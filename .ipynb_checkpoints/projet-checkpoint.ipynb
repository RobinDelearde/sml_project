{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning for sentiment classification on movie reviews\n",
    "This project aims at compare several classification algorithms and textual data representations with their parameters, on a dataset of movies reviews classified as positive or negative.\n",
    "We experiment a representation as token counts (CountVectorizer) and 4 classification algorithms:\n",
    " - Multinomial Naive Bayes\n",
    " - Logistic regression\n",
    " - SVM\n",
    " - bi-directionnal LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load common librairies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "First the data is transformed into pandas dataframes by some file manipulation. We use the Polarity dataset v2.0 of [Bo Pang and Lillian Lee, ACL 2004], which is available at: http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dedicated library\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing: prepare data\n",
    "col_names = ['content', 'label']\n",
    "pos = pd.DataFrame(columns = col_names)\n",
    "neg = pd.DataFrame(columns = col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add positive samples to the DataFrame structure\n",
    "i=1\n",
    "for fend in os.listdir('./dataset1/pos/'):\n",
    "    #data = pd.read_csv('./dataset1/pos/'+fend, sep = None, header = None)\n",
    "    file = open('./dataset1/pos/'+fend, 'r')\n",
    "    data = file.read()\n",
    "    #print(data)\n",
    "    file.close()\n",
    "    pos = pos.append(pd.DataFrame({'content':[data], 'label':int(1)}, index=[i]))\n",
    "    i+=1\n",
    "# add negative samples to the DataFrame structure\n",
    "i=1\n",
    "for fend in os.listdir('./dataset1/neg/'):\n",
    "    #data = pd.read_csv('./dataset1/neg/'+fend, sep = None, header = None)\n",
    "    file = open('./dataset1/neg/'+fend, 'r')\n",
    "    data = file.read()\n",
    "    file.close()\n",
    "    neg = neg.append(pd.DataFrame({'content':[data],'label':int(-1)}, index=[i]))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert label to a numerical variable\n",
    "#reviews[\"label_num\"] = reviews.label.map({\"1\":int(1), \"-1\":int(0)})\n",
    "pos[\"label_num\"] = pos['label'].astype(int)\n",
    "neg[\"label_num\"] = neg['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-processing done\n",
      "number of positive samples: 1000 \n",
      "number of negative samples: 1000 \n",
      "\n",
      "last positive samples:\n",
      "                                                content label  label_num\n",
      "996   now , lets first look into the history of shar...     1          1\n",
      "997   call 911 for the cliche police if you must , b...     1          1\n",
      "998   is it just me , or have disney films gradually...     1          1\n",
      "999   it must be some sort of warped critical nightm...     1          1\n",
      "1000  those print and television ads trumpeting that...     1          1\n",
      "\n",
      "last negative samples:\n",
      "                                                content label  label_num\n",
      "996   synopsis : al simmons , top-notch assasin with...    -1         -1\n",
      "997   the kids in the hall are an acquired taste . \\...    -1         -1\n",
      "998   david spade has a snide , sarcastic sense of h...    -1         -1\n",
      "999   i've never written a review for a movie i have...    -1         -1\n",
      "1000  deceiver is a plot twist in search of a movie ...    -1         -1\n"
     ]
    }
   ],
   "source": [
    "# print\n",
    "print('pre-processing done')\n",
    "print('number of positive samples: {} '.format(len(pos)))\n",
    "print('number of negative samples: {} '.format(len(neg)))\n",
    "print()\n",
    "print('last positive samples:')\n",
    "print(pos.tail(5))\n",
    "print()\n",
    "print('last negative samples:')\n",
    "print(neg.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare learning datasets\n",
    "Then we mix the 2 dataframes of positive and negative samples into 2 random dataframes dedicated to training and testing. We verified that the random split was always done in the same way (with the same seed) to ensure that the results are the same every time we run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat positive and negative samples\n",
    "reviews = pos.append(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X (items) and y (labels)\n",
    "X = reviews.content\n",
    "y = reviews.label_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split randomly X and y into train and test sets (NB: always uses the same seed)\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train samples: 1500\n",
      "number of test samples: 500\n",
      "\n",
      "first train samples with their labels:\n",
      "651    after bloody clashes and independence won , lu...\n",
      "105    alexander dumas' the three musketeers is one o...\n",
      "562    no , i did not read the novel by thomas hardy ...\n",
      "644    billy bob thornton , who had a sudden rise to ...\n",
      "442    george little ( jonathan lipnicki ) wants a li...\n",
      "629    fantastically over hyped , godzila finally lum...\n",
      "997    the kids in the hall are an acquired taste . \\...\n",
      "681    titantic , writer and director james cameron's...\n",
      "813    synopsis : captain picard and the crew of the ...\n",
      "505    i think maybe it's time for the batman series ...\n",
      "Name: content, dtype: object\n",
      "651    1\n",
      "105   -1\n",
      "562    1\n",
      "644    1\n",
      "442    1\n",
      "629   -1\n",
      "997   -1\n",
      "681    1\n",
      "813    1\n",
      "505   -1\n",
      "Name: label_num, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print\n",
    "print('number of train samples: {}'.format(len(X_train)))\n",
    "print('number of test samples: {}'.format(len(X_test)))\n",
    "print()\n",
    "print('first train samples with their labels:')\n",
    "print(X_train.head(10))\n",
    "print(y_train.head(10))\n",
    "#print()\n",
    "#print('first test samples with their labels:')\n",
    "#print(X_test.head(10))\n",
    "#print(y_test.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Representing text as numerical data (vectorization)\n",
    "In order to use text as input of the classification algorithms, we need to convert it into fixed-size numerical feature vectors. To do so we use scikit-learn CountVectorizer that \"converts text into a matrix of token counts\", or \"bag of words\". It first learns the vocabulary over the text given as input, and then transforms any textual data into a document-term matrix using the fitted vocabulary. We learn the vocabulary on the training dataset, and compute the document-term matrix representations for both training and testing data. The basic method uses the input token as such in the vocabulary, but it's also possible to use methods of natural language processing to use more elaborated tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and instantiate CountVectorizer (with the default parameters)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer(min_df=10, max_df=0.33, ngram_range=(1, 3), stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.33, max_features=None, min_df=10,\n",
       "        ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learn the \"vocabulary\" of the training data (occurs in-place)\n",
    "vect.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words in the vocabulary: 7836 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['000',\n",
       " '10',\n",
       " '10 minutes',\n",
       " '10 scale',\n",
       " '10 scale scale',\n",
       " '100',\n",
       " '100 million',\n",
       " '1000',\n",
       " '101',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '13th',\n",
       " '14',\n",
       " '15',\n",
       " '15 minutes',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '1950',\n",
       " '1950s',\n",
       " '1960',\n",
       " '1960s',\n",
       " '1970',\n",
       " '1970s',\n",
       " '1971',\n",
       " '1972',\n",
       " '1973',\n",
       " '1977',\n",
       " '1978',\n",
       " '1979',\n",
       " '1980',\n",
       " '1980s',\n",
       " '1981',\n",
       " '1983',\n",
       " '1984',\n",
       " '1985',\n",
       " '1986',\n",
       " '1987',\n",
       " '1988',\n",
       " '1989',\n",
       " '1990',\n",
       " '1990s',\n",
       " '1991',\n",
       " '1992',\n",
       " '1993',\n",
       " '1994',\n",
       " '1995',\n",
       " '1996',\n",
       " '1997',\n",
       " '1998',\n",
       " '1999',\n",
       " '19th',\n",
       " '19th century',\n",
       " '20',\n",
       " '20 minutes',\n",
       " '200',\n",
       " '2000',\n",
       " '2001',\n",
       " '2001 space',\n",
       " '2001 space odyssey',\n",
       " '20th',\n",
       " '20th century',\n",
       " '21',\n",
       " '22',\n",
       " '24',\n",
       " '25',\n",
       " '30',\n",
       " '30 minutes',\n",
       " '3000',\n",
       " '35',\n",
       " '40',\n",
       " '45',\n",
       " '50',\n",
       " '50s',\n",
       " '60',\n",
       " '60s',\n",
       " '70',\n",
       " '70s',\n",
       " '80',\n",
       " '80s',\n",
       " '85',\n",
       " '90',\n",
       " '90 minute',\n",
       " '90 minutes',\n",
       " '90210',\n",
       " '90s',\n",
       " '95',\n",
       " '97',\n",
       " '99',\n",
       " 'aaron',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'able make',\n",
       " 'ably',\n",
       " 'aboard',\n",
       " 'abound',\n",
       " 'absence',\n",
       " 'absent',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absorbed',\n",
       " 'absorbing',\n",
       " 'absurd',\n",
       " 'absurdity',\n",
       " 'abundance',\n",
       " 'abuse',\n",
       " 'abusive',\n",
       " 'abyss',\n",
       " 'academy',\n",
       " 'academy award',\n",
       " 'academy awards',\n",
       " 'accent',\n",
       " 'accents',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'acceptance',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'accepts',\n",
       " 'access',\n",
       " 'accident',\n",
       " 'accidentally',\n",
       " 'acclaimed',\n",
       " 'accompanied',\n",
       " 'accompany',\n",
       " 'accompanying',\n",
       " 'accomplish',\n",
       " 'accomplished',\n",
       " 'accomplishment',\n",
       " 'according',\n",
       " 'account',\n",
       " 'accuracy',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'accused',\n",
       " 'ace',\n",
       " 'ace ventura',\n",
       " 'achieve',\n",
       " 'achieved',\n",
       " 'achievement',\n",
       " 'achieves',\n",
       " 'achieving',\n",
       " 'acid',\n",
       " 'act',\n",
       " 'act like',\n",
       " 'acted',\n",
       " 'acting',\n",
       " 'acting film',\n",
       " 'acting like',\n",
       " 'acting skills',\n",
       " 'action',\n",
       " 'action adventure',\n",
       " 'action comedy',\n",
       " 'action director',\n",
       " 'action film',\n",
       " 'action films',\n",
       " 'action flick',\n",
       " 'action hero',\n",
       " 'action movie',\n",
       " 'action movies',\n",
       " 'action packed',\n",
       " 'action scenes',\n",
       " 'action sequence',\n",
       " 'action sequences',\n",
       " 'action thriller',\n",
       " 'actions',\n",
       " 'activities',\n",
       " 'actor',\n",
       " 'actors',\n",
       " 'actors film',\n",
       " 'actors like',\n",
       " 'actress',\n",
       " 'actresses',\n",
       " 'acts',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'actually quite',\n",
       " 'ad',\n",
       " 'adam',\n",
       " 'adam sandler',\n",
       " 'adams',\n",
       " 'adaptation',\n",
       " 'adaptations',\n",
       " 'adapted',\n",
       " 'add',\n",
       " 'added',\n",
       " 'addict',\n",
       " 'addicted',\n",
       " 'addiction',\n",
       " 'adding',\n",
       " 'addition',\n",
       " 'additionally',\n",
       " 'addresses',\n",
       " 'adds',\n",
       " 'adept',\n",
       " 'adequate',\n",
       " 'adequately',\n",
       " 'admirable',\n",
       " 'admirably',\n",
       " 'admiration',\n",
       " 'admire',\n",
       " 'admired',\n",
       " 'admission',\n",
       " 'admit',\n",
       " 'admits',\n",
       " 'admittedly',\n",
       " 'adolescent',\n",
       " 'adopts',\n",
       " 'adorable',\n",
       " 'adrenaline',\n",
       " 'ads',\n",
       " 'adult',\n",
       " 'adults',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advances',\n",
       " 'advantage',\n",
       " 'adventure',\n",
       " 'adventures',\n",
       " 'advertised',\n",
       " 'advertising',\n",
       " 'advice',\n",
       " 'advised',\n",
       " 'advisor',\n",
       " 'advocate',\n",
       " 'affable',\n",
       " 'affair',\n",
       " 'affairs',\n",
       " 'affect',\n",
       " 'affected',\n",
       " 'affecting',\n",
       " 'affection',\n",
       " 'affleck',\n",
       " 'aforementioned',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'african american',\n",
       " 'afternoon',\n",
       " 'afterthought',\n",
       " 'afterward',\n",
       " 'age',\n",
       " 'aged',\n",
       " 'agency',\n",
       " 'agenda',\n",
       " 'agent',\n",
       " 'agents',\n",
       " 'ages',\n",
       " 'aggressive',\n",
       " 'aging',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agreeable',\n",
       " 'agrees',\n",
       " 'ahead',\n",
       " 'aid',\n",
       " 'aided',\n",
       " 'aids',\n",
       " 'aim',\n",
       " 'aimed',\n",
       " 'aims',\n",
       " 'ain',\n",
       " 'air',\n",
       " 'air force',\n",
       " 'airplane',\n",
       " 'airport',\n",
       " 'akin',\n",
       " 'al',\n",
       " 'al pacino',\n",
       " 'alan',\n",
       " 'alas',\n",
       " 'albeit',\n",
       " 'albert',\n",
       " 'alcoholic',\n",
       " 'alec',\n",
       " 'alex',\n",
       " 'alexander',\n",
       " 'alfred',\n",
       " 'alfred hitchcock',\n",
       " 'ali',\n",
       " 'ali larter',\n",
       " 'alice',\n",
       " 'alicia',\n",
       " 'alicia silverstone',\n",
       " 'alien',\n",
       " 'aliens',\n",
       " 'alike',\n",
       " 'alive',\n",
       " 'allegedly',\n",
       " 'allen',\n",
       " 'alley',\n",
       " 'allow',\n",
       " 'allowed',\n",
       " 'allowing',\n",
       " 'allows',\n",
       " 'alluring',\n",
       " 'ally',\n",
       " 'alongside',\n",
       " 'alright',\n",
       " 'alter',\n",
       " 'altered',\n",
       " 'alternate',\n",
       " 'alternative',\n",
       " 'altman',\n",
       " 'altogether',\n",
       " 'amanda',\n",
       " 'amateurish',\n",
       " 'amazed',\n",
       " 'amazing',\n",
       " 'amazing potent',\n",
       " 'amazing potent stuff',\n",
       " 'amazingly',\n",
       " 'ambassador',\n",
       " 'ambiguity',\n",
       " 'ambiguous',\n",
       " 'ambition',\n",
       " 'ambitions',\n",
       " 'ambitious',\n",
       " 'america',\n",
       " 'american',\n",
       " 'american beauty',\n",
       " 'american film',\n",
       " 'american history',\n",
       " 'american pie',\n",
       " 'americans',\n",
       " 'amidst',\n",
       " 'amounts',\n",
       " 'amusement',\n",
       " 'amusing',\n",
       " 'amy',\n",
       " 'anaconda',\n",
       " 'analysis',\n",
       " 'analyze',\n",
       " 'ancient',\n",
       " 'anderson',\n",
       " 'andrew',\n",
       " 'andy',\n",
       " 'angel',\n",
       " 'angela',\n",
       " 'angeles',\n",
       " 'angelina',\n",
       " 'angelina jolie',\n",
       " 'angels',\n",
       " 'anger',\n",
       " 'angle',\n",
       " 'angles',\n",
       " 'angry',\n",
       " 'angst',\n",
       " 'animal',\n",
       " 'animals',\n",
       " 'animated',\n",
       " 'animated feature',\n",
       " 'animated film',\n",
       " 'animated films',\n",
       " 'animation',\n",
       " 'animators',\n",
       " 'ann',\n",
       " 'anna',\n",
       " 'anne',\n",
       " 'annette',\n",
       " 'annette bening',\n",
       " 'annie',\n",
       " 'anniversary',\n",
       " 'announces',\n",
       " 'annoyed',\n",
       " 'annoying',\n",
       " 'annual',\n",
       " 'anonymous',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'answers',\n",
       " 'anthony',\n",
       " 'anthony hopkins',\n",
       " 'anti',\n",
       " 'anticipated',\n",
       " 'anticipation',\n",
       " 'antics',\n",
       " 'antonio',\n",
       " 'antonio banderas',\n",
       " 'anybody',\n",
       " 'anymore',\n",
       " 'apart',\n",
       " 'apartment',\n",
       " 'ape',\n",
       " 'apes',\n",
       " 'apocalypse',\n",
       " 'apollo',\n",
       " 'apollo 13',\n",
       " 'appalling',\n",
       " 'apparent',\n",
       " 'apparently',\n",
       " 'appeal',\n",
       " 'appealing',\n",
       " 'appear',\n",
       " 'appearance',\n",
       " 'appearances',\n",
       " 'appeared',\n",
       " 'appearing',\n",
       " 'appears',\n",
       " 'apple',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'approach',\n",
       " 'approached',\n",
       " 'approaches',\n",
       " 'approaching',\n",
       " 'appropriate',\n",
       " 'appropriately',\n",
       " 'approximately',\n",
       " 'apt',\n",
       " 'aptly',\n",
       " 'arc',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'aren',\n",
       " 'arguably',\n",
       " 'argue',\n",
       " 'arguing',\n",
       " 'argument',\n",
       " 'arguments',\n",
       " 'arise',\n",
       " 'arises',\n",
       " 'arizona',\n",
       " 'ark',\n",
       " 'arm',\n",
       " 'armageddon',\n",
       " 'armed',\n",
       " 'armor',\n",
       " 'arms',\n",
       " 'army',\n",
       " 'arnold',\n",
       " 'arnold schwarzenegger',\n",
       " 'arquette',\n",
       " 'array',\n",
       " 'arrest',\n",
       " 'arrested',\n",
       " 'arrival',\n",
       " 'arrive',\n",
       " 'arrived',\n",
       " 'arrives',\n",
       " 'arriving',\n",
       " 'arrogance',\n",
       " 'arrogant',\n",
       " 'arrow',\n",
       " 'art',\n",
       " 'art direction',\n",
       " 'arthur',\n",
       " 'article',\n",
       " 'artificial',\n",
       " 'artist',\n",
       " 'artistic',\n",
       " 'artists',\n",
       " 'arts',\n",
       " 'ashley',\n",
       " 'ashley judd',\n",
       " 'asian',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'asks',\n",
       " 'asleep',\n",
       " 'aspect',\n",
       " 'aspect film',\n",
       " 'aspect ratio',\n",
       " 'aspects',\n",
       " 'aspects film',\n",
       " 'aspiring',\n",
       " 'ass',\n",
       " 'assassin',\n",
       " 'assassination',\n",
       " 'assault',\n",
       " 'assembled',\n",
       " 'asset',\n",
       " 'assigned',\n",
       " 'assignment',\n",
       " 'assistance',\n",
       " 'assistant',\n",
       " 'associate',\n",
       " 'associated',\n",
       " 'assortment',\n",
       " 'assume',\n",
       " 'assumes',\n",
       " 'assuming',\n",
       " 'assured',\n",
       " 'astonishing',\n",
       " 'astounding',\n",
       " 'astronaut',\n",
       " 'atlantic',\n",
       " 'atmosphere',\n",
       " 'atom',\n",
       " 'atrocious',\n",
       " 'attached',\n",
       " 'attack',\n",
       " 'attacked',\n",
       " 'attacking',\n",
       " 'attacks',\n",
       " 'attempt',\n",
       " 'attempted',\n",
       " 'attempting',\n",
       " 'attempts',\n",
       " 'attend',\n",
       " 'attendant',\n",
       " 'attended',\n",
       " 'attention',\n",
       " 'attitude',\n",
       " 'attitudes',\n",
       " 'attorney',\n",
       " 'attract',\n",
       " 'attracted',\n",
       " 'attraction',\n",
       " 'attractive',\n",
       " 'audience',\n",
       " 'audience film',\n",
       " 'audience members',\n",
       " 'audiences',\n",
       " 'audio',\n",
       " 'august',\n",
       " 'austin',\n",
       " 'austin powers',\n",
       " 'australia',\n",
       " 'australian',\n",
       " 'auteur',\n",
       " 'authentic',\n",
       " 'authenticity',\n",
       " 'author',\n",
       " 'authorities',\n",
       " 'authority',\n",
       " 'auto',\n",
       " 'automatically',\n",
       " 'available',\n",
       " 'available dvd',\n",
       " 'average',\n",
       " 'avoid',\n",
       " 'avoided',\n",
       " 'avoiding',\n",
       " 'avoids',\n",
       " 'awaiting',\n",
       " 'awake',\n",
       " 'award',\n",
       " 'award winning',\n",
       " 'awards',\n",
       " 'aware',\n",
       " 'away',\n",
       " 'away film',\n",
       " 'away movie',\n",
       " 'awe',\n",
       " 'awesome',\n",
       " 'awful',\n",
       " 'awfully',\n",
       " 'awhile',\n",
       " 'awkward',\n",
       " 'awkwardly',\n",
       " 'awry',\n",
       " 'azaria',\n",
       " 'babe',\n",
       " 'baby',\n",
       " 'bachelor',\n",
       " 'backdrop',\n",
       " 'background',\n",
       " 'bacon',\n",
       " 'bad acting',\n",
       " 'bad dialogue',\n",
       " 'bad film',\n",
       " 'bad good',\n",
       " 'bad guy',\n",
       " 'bad guys',\n",
       " 'bad movie',\n",
       " 'bad movies',\n",
       " 'bad news',\n",
       " 'bad thing',\n",
       " 'bad things',\n",
       " 'badly',\n",
       " 'bag',\n",
       " 'baked',\n",
       " 'baker',\n",
       " 'balance',\n",
       " 'bald',\n",
       " 'baldwin',\n",
       " 'ball',\n",
       " 'balls',\n",
       " 'banal',\n",
       " 'band',\n",
       " 'banderas',\n",
       " 'bang',\n",
       " 'bank',\n",
       " 'banks',\n",
       " 'banter',\n",
       " 'bar',\n",
       " 'barbara',\n",
       " 'barber',\n",
       " 'bare',\n",
       " 'barely',\n",
       " 'barnes',\n",
       " 'barrage',\n",
       " 'barrel',\n",
       " 'barry',\n",
       " 'barrymore',\n",
       " 'bars',\n",
       " 'bartender',\n",
       " 'base',\n",
       " 'baseball',\n",
       " 'based',\n",
       " 'based book',\n",
       " 'based novel',\n",
       " 'based true',\n",
       " 'based true story',\n",
       " 'basement',\n",
       " 'basic',\n",
       " 'basic instinct',\n",
       " 'basic plot',\n",
       " 'basically',\n",
       " 'basically just',\n",
       " 'basis',\n",
       " 'basketball',\n",
       " 'bastard',\n",
       " 'bat',\n",
       " 'bates',\n",
       " 'bathroom',\n",
       " 'bathtub',\n",
       " 'batman',\n",
       " 'batman robin',\n",
       " 'battle',\n",
       " 'battle scenes',\n",
       " 'battlefield',\n",
       " 'battles',\n",
       " 'battling',\n",
       " 'bay',\n",
       " 'beach',\n",
       " 'bean',\n",
       " 'bear',\n",
       " 'bears',\n",
       " 'beast',\n",
       " 'beat',\n",
       " 'beaten',\n",
       " 'beating',\n",
       " 'beats',\n",
       " 'beatty',\n",
       " 'beautiful',\n",
       " 'beautiful woman',\n",
       " 'beautifully',\n",
       " 'beauty',\n",
       " 'bed',\n",
       " 'bedroom',\n",
       " 'beer',\n",
       " 'befriends',\n",
       " 'began',\n",
       " 'begin',\n",
       " 'beginning',\n",
       " 'beginning end',\n",
       " 'beginning film',\n",
       " 'begins',\n",
       " 'begs',\n",
       " 'begun',\n",
       " 'behavior',\n",
       " 'behold',\n",
       " 'beings',\n",
       " 'belief',\n",
       " 'beliefs',\n",
       " 'believability',\n",
       " 'believable',\n",
       " 'believe',\n",
       " 'believed',\n",
       " 'believes',\n",
       " 'believing',\n",
       " 'bell',\n",
       " 'belly',\n",
       " 'belong',\n",
       " 'belonging',\n",
       " 'belongs',\n",
       " 'beloved',\n",
       " 'belt',\n",
       " 'ben',\n",
       " 'ben affleck',\n",
       " 'ben stiller',\n",
       " 'beneath',\n",
       " 'benefit',\n",
       " 'benefits',\n",
       " 'bening',\n",
       " 'bent',\n",
       " 'bernard',\n",
       " 'best actor',\n",
       " 'best described',\n",
       " 'best film',\n",
       " 'best films',\n",
       " 'best films year',\n",
       " 'best friend',\n",
       " 'best friends',\n",
       " 'best known',\n",
       " 'best movie',\n",
       " 'best movies',\n",
       " 'best performance',\n",
       " 'best performances',\n",
       " 'best picture',\n",
       " 'best scene',\n",
       " 'best supporting',\n",
       " 'best thing',\n",
       " 'best way',\n",
       " 'best work',\n",
       " 'bet',\n",
       " 'betrayal',\n",
       " 'better film',\n",
       " 'better movie',\n",
       " 'better staying',\n",
       " 'better staying home',\n",
       " 'betty',\n",
       " 'beverly',\n",
       " 'beverly hills',\n",
       " 'beware',\n",
       " 'bible',\n",
       " 'bickering',\n",
       " 'big',\n",
       " 'big budget',\n",
       " 'big fan',\n",
       " 'big screen',\n",
       " 'big stars',\n",
       " 'big time',\n",
       " 'bigger',\n",
       " 'biggest',\n",
       " 'biggest problem',\n",
       " 'biggs',\n",
       " 'billed',\n",
       " 'billing',\n",
       " 'billy',\n",
       " 'billy bob',\n",
       " 'billy bob thornton',\n",
       " 'billy crystal',\n",
       " 'bird',\n",
       " 'birds',\n",
       " 'birth',\n",
       " 'birthday',\n",
       " 'bit',\n",
       " 'bitch',\n",
       " 'bite',\n",
       " 'biting',\n",
       " 'bits',\n",
       " 'bitter',\n",
       " 'bizarre',\n",
       " 'black',\n",
       " 'black comedy',\n",
       " 'black white',\n",
       " 'blacks',\n",
       " 'blade',\n",
       " 'blade runner',\n",
       " 'blair',\n",
       " 'blair witch',\n",
       " 'blair witch project',\n",
       " 'blake',\n",
       " 'blame',\n",
       " 'bland',\n",
       " 'blank',\n",
       " 'blast',\n",
       " 'blatant',\n",
       " 'blatantly',\n",
       " 'bleak',\n",
       " 'blend',\n",
       " 'blessed',\n",
       " 'blind',\n",
       " 'block',\n",
       " 'blockbuster',\n",
       " 'blond',\n",
       " 'blonde',\n",
       " 'blood',\n",
       " 'blood gore',\n",
       " 'bloody',\n",
       " 'blow',\n",
       " 'blowing',\n",
       " 'blown',\n",
       " 'blows',\n",
       " 'blue',\n",
       " 'blues',\n",
       " 'board',\n",
       " 'boasts',\n",
       " 'boat',\n",
       " 'boats',\n",
       " 'bob',\n",
       " 'bob thornton',\n",
       " 'bobby',\n",
       " 'bodies',\n",
       " 'body',\n",
       " 'body count',\n",
       " 'boiled',\n",
       " 'boils',\n",
       " 'bold',\n",
       " 'bomb',\n",
       " 'bond',\n",
       " 'bone',\n",
       " 'bones',\n",
       " 'bont',\n",
       " 'bonus',\n",
       " 'boogie',\n",
       " 'boogie nights',\n",
       " 'book',\n",
       " 'book film',\n",
       " 'books',\n",
       " 'bookstore',\n",
       " 'boot',\n",
       " 'border',\n",
       " 'borders',\n",
       " 'bore',\n",
       " 'bored',\n",
       " 'boredom',\n",
       " 'boring',\n",
       " 'born',\n",
       " 'borrowed',\n",
       " 'borrows',\n",
       " 'boss',\n",
       " 'boston',\n",
       " 'bother',\n",
       " 'bothered',\n",
       " 'bottle',\n",
       " 'bought',\n",
       " 'bound',\n",
       " 'bow',\n",
       " 'bowl',\n",
       " 'bowling',\n",
       " 'box',\n",
       " 'box office',\n",
       " 'boxer',\n",
       " 'boxing',\n",
       " 'boy',\n",
       " 'boyfriend',\n",
       " 'boyle',\n",
       " 'boys',\n",
       " 'brad',\n",
       " 'brad pitt',\n",
       " 'brain',\n",
       " 'brainless',\n",
       " 'brains',\n",
       " 'branagh',\n",
       " 'brand',\n",
       " 'brando',\n",
       " 'brave',\n",
       " 'braveheart',\n",
       " 'brazil',\n",
       " 'break',\n",
       " 'breakdown',\n",
       " 'breakfast',\n",
       " 'breaking',\n",
       " 'breaks',\n",
       " 'breakthrough',\n",
       " 'breasts',\n",
       " 'breath',\n",
       " 'breathing',\n",
       " 'breathtaking',\n",
       " 'breed',\n",
       " 'brian',\n",
       " 'brian palma',\n",
       " 'bride',\n",
       " 'bridge',\n",
       " 'bridges',\n",
       " 'bridget',\n",
       " 'bridget fonda',\n",
       " 'brief',\n",
       " 'briefly',\n",
       " 'bright',\n",
       " 'brilliance',\n",
       " 'brilliant',\n",
       " 'brilliantly',\n",
       " 'bring',\n",
       " 'bring friend',\n",
       " 'bring friend amazing',\n",
       " 'bringing',\n",
       " 'brings',\n",
       " 'brink',\n",
       " 'british',\n",
       " 'broad',\n",
       " 'broadcast',\n",
       " 'broadway',\n",
       " 'broderick',\n",
       " 'broke',\n",
       " 'broken',\n",
       " 'brooding',\n",
       " 'brooklyn',\n",
       " 'brooks',\n",
       " 'bros',\n",
       " 'brosnan',\n",
       " 'brother',\n",
       " 'brothers',\n",
       " 'brought',\n",
       " 'brown',\n",
       " 'bruce',\n",
       " 'bruce greenwood',\n",
       " 'bruce willis',\n",
       " 'brutal',\n",
       " 'brutality',\n",
       " 'brutally',\n",
       " 'bubble',\n",
       " 'buck',\n",
       " 'bucks',\n",
       " 'bud',\n",
       " 'buddies',\n",
       " 'buddy',\n",
       " 'budget',\n",
       " 'budget film',\n",
       " 'buff',\n",
       " 'bug',\n",
       " 'bugs',\n",
       " 'build',\n",
       " 'building',\n",
       " 'buildings',\n",
       " 'builds',\n",
       " 'built',\n",
       " 'bulk',\n",
       " 'bull',\n",
       " 'bullet',\n",
       " 'bullets',\n",
       " 'bullock',\n",
       " 'bumbling',\n",
       " 'bunch',\n",
       " 'bunny',\n",
       " 'buried',\n",
       " 'burn',\n",
       " 'burned',\n",
       " 'burning',\n",
       " 'burns',\n",
       " 'burst',\n",
       " 'burt',\n",
       " 'burton',\n",
       " 'bus',\n",
       " 'buscemi',\n",
       " 'busey',\n",
       " 'business',\n",
       " 'businessman',\n",
       " 'bust',\n",
       " 'busy',\n",
       " 'butler',\n",
       " 'butt',\n",
       " 'button',\n",
       " 'buy',\n",
       " 'buying',\n",
       " 'buzz',\n",
       " 'byrne',\n",
       " 'caan',\n",
       " 'cab',\n",
       " 'cabin',\n",
       " 'cable',\n",
       " 'cage',\n",
       " 'cake',\n",
       " 'calculated',\n",
       " 'caliber',\n",
       " 'california',\n",
       " 'called',\n",
       " 'calling',\n",
       " 'calls',\n",
       " 'calm',\n",
       " 'came',\n",
       " 'cameo',\n",
       " 'cameos',\n",
       " 'camera',\n",
       " 'camera angles',\n",
       " 'camera work',\n",
       " 'cameraman',\n",
       " 'cameras',\n",
       " 'camerawork',\n",
       " 'cameron',\n",
       " 'cameron diaz',\n",
       " 'camp',\n",
       " 'campaign',\n",
       " 'campbell',\n",
       " 'campus',\n",
       " 'campy',\n",
       " 'canadian',\n",
       " 'cancer',\n",
       " 'candidate',\n",
       " 'candy',\n",
       " 'capable',\n",
       " 'capacity',\n",
       " 'cape',\n",
       " 'caper',\n",
       " 'capital',\n",
       " 'capitalize',\n",
       " 'capsule',\n",
       " 'captain',\n",
       " 'captivating',\n",
       " 'capture',\n",
       " 'captured',\n",
       " 'captures',\n",
       " 'capturing',\n",
       " 'car',\n",
       " 'car accident',\n",
       " 'car chase',\n",
       " 'car chases',\n",
       " 'card',\n",
       " 'cardboard',\n",
       " 'cardinal',\n",
       " 'cards',\n",
       " 'care',\n",
       " 'care characters',\n",
       " 'cared',\n",
       " 'career',\n",
       " 'careers',\n",
       " 'careful',\n",
       " 'carefully',\n",
       " 'cares',\n",
       " ...]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the fitted vocabulary\n",
    "vocabulary = vect.get_feature_names()\n",
    "print('number of words in the vocabulary: {} '.format(len(vocabulary)))\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1500x7836 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 299857 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform training data into a \"document-term matrix'\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "X_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 78)\t1\n",
      "  (0, 95)\t1\n",
      "  (0, 172)\t1\n",
      "  (0, 317)\t1\n",
      "  (0, 324)\t1\n",
      "  (0, 434)\t1\n",
      "  (0, 489)\t1\n",
      "  (0, 769)\t1\n",
      "  (0, 904)\t1\n",
      "  (0, 1034)\t1\n",
      "  (0, 1050)\t1\n",
      "  (0, 1228)\t1\n",
      "  (0, 1284)\t1\n",
      "  (0, 1360)\t1\n",
      "  (0, 1432)\t1\n",
      "  (0, 1444)\t1\n",
      "  (0, 1516)\t2\n",
      "  (0, 1804)\t1\n",
      "  (0, 1905)\t1\n",
      "  (0, 2010)\t1\n",
      "  (0, 2131)\t1\n",
      "  (0, 2514)\t1\n",
      "  (0, 2533)\t1\n",
      "  (0, 2573)\t1\n",
      "  (0, 2593)\t1\n",
      "  :\t:\n",
      "  (1499, 5891)\t1\n",
      "  (1499, 5906)\t2\n",
      "  (1499, 5967)\t2\n",
      "  (1499, 5996)\t2\n",
      "  (1499, 6004)\t1\n",
      "  (1499, 6153)\t1\n",
      "  (1499, 6357)\t1\n",
      "  (1499, 6359)\t1\n",
      "  (1499, 6386)\t1\n",
      "  (1499, 6400)\t1\n",
      "  (1499, 6568)\t1\n",
      "  (1499, 6652)\t1\n",
      "  (1499, 6826)\t1\n",
      "  (1499, 6871)\t1\n",
      "  (1499, 6963)\t1\n",
      "  (1499, 7195)\t1\n",
      "  (1499, 7286)\t1\n",
      "  (1499, 7376)\t1\n",
      "  (1499, 7507)\t1\n",
      "  (1499, 7513)\t1\n",
      "  (1499, 7583)\t1\n",
      "  (1499, 7599)\t1\n",
      "  (1499, 7619)\t1\n",
      "  (1499, 7620)\t1\n",
      "  (1499, 7688)\t1\n"
     ]
    }
   ],
   "source": [
    "# examine the content of the sparse matrix\n",
    "print(X_train_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>10 minutes</th>\n",
       "      <th>10 scale</th>\n",
       "      <th>10 scale scale</th>\n",
       "      <th>100</th>\n",
       "      <th>100 million</th>\n",
       "      <th>1000</th>\n",
       "      <th>101</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>younger brother</th>\n",
       "      <th>youth</th>\n",
       "      <th>zane</th>\n",
       "      <th>zany</th>\n",
       "      <th>zero</th>\n",
       "      <th>zeta</th>\n",
       "      <th>zeta jones</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zooms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1470</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1471</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1473</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1475</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1477</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1478</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1480</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1483</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1491</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1493</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1494</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows  7836 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      000  10  10 minutes  10 scale  10 scale scale  100  100 million  1000  \\\n",
       "0       0   0           0         0               0    0            0     0   \n",
       "1       0   0           0         0               0    0            0     0   \n",
       "2       0   0           0         0               0    0            0     0   \n",
       "3       0   0           0         0               0    0            0     0   \n",
       "4       0   0           0         0               0    0            0     0   \n",
       "5       0   0           0         0               0    0            0     0   \n",
       "6       0   0           0         0               0    0            0     0   \n",
       "7       2   0           0         0               0    0            0     0   \n",
       "8       0   0           0         0               0    0            0     0   \n",
       "9       1   0           0         0               0    0            0     0   \n",
       "10      0   0           0         0               0    0            0     0   \n",
       "11      0   0           0         0               0    0            0     0   \n",
       "12      0   0           0         0               0    0            0     0   \n",
       "13      0   0           0         0               0    0            0     0   \n",
       "14      0   0           0         0               0    0            0     0   \n",
       "15      0   0           0         0               0    0            0     0   \n",
       "16      0   0           0         0               0    0            0     0   \n",
       "17      0   0           0         0               0    0            0     0   \n",
       "18      0   0           0         0               0    0            0     0   \n",
       "19      0   0           0         0               0    0            0     0   \n",
       "20      0   0           0         0               0    0            0     0   \n",
       "21      0   0           0         0               0    0            0     0   \n",
       "22      0   0           0         0               0    0            0     0   \n",
       "23      0   0           0         0               0    0            0     0   \n",
       "24      0   0           0         0               0    0            0     0   \n",
       "25      0   0           0         0               0    0            0     0   \n",
       "26      0   0           0         0               0    0            0     0   \n",
       "27      0   0           0         0               0    0            0     0   \n",
       "28      1   0           0         0               0    1            0     0   \n",
       "29      0   0           0         0               0    0            0     0   \n",
       "...   ...  ..         ...       ...             ...  ...          ...   ...   \n",
       "1470    0   0           0         0               0    0            0     0   \n",
       "1471    0   0           0         0               0    0            0     0   \n",
       "1472    0   0           0         0               0    0            0     0   \n",
       "1473    0   0           0         0               0    0            0     0   \n",
       "1474    0   0           0         0               0    0            0     0   \n",
       "1475    0   0           0         0               0    0            0     0   \n",
       "1476    0   0           0         0               0    0            0     0   \n",
       "1477    0   0           0         0               0    0            0     0   \n",
       "1478    1   1           0         0               0    0            0     0   \n",
       "1479    0   0           0         0               0    0            0     0   \n",
       "1480    0   0           0         0               0    0            0     0   \n",
       "1481    0   0           0         0               0    0            0     0   \n",
       "1482    0   0           0         0               0    0            0     0   \n",
       "1483    0   0           0         0               0    0            0     0   \n",
       "1484    0   0           0         0               0    0            0     0   \n",
       "1485    0   0           0         0               0    0            0     0   \n",
       "1486    0   0           0         0               0    0            0     0   \n",
       "1487    0   0           0         0               0    0            0     0   \n",
       "1488    2   2           0         0               0    0            0     0   \n",
       "1489    0   0           0         0               0    0            0     0   \n",
       "1490    0   0           0         0               0    0            0     0   \n",
       "1491    0   0           0         0               0    0            0     0   \n",
       "1492    0   0           0         0               0    0            0     0   \n",
       "1493    0   0           0         0               0    0            0     0   \n",
       "1494    1   2           0         0               0    0            0     0   \n",
       "1495    0   0           0         0               0    0            0     0   \n",
       "1496    0   1           0         0               0    0            0     0   \n",
       "1497    0   0           0         0               0    0            0     0   \n",
       "1498    0   0           0         0               0    0            0     0   \n",
       "1499    0   0           0         0               0    0            0     0   \n",
       "\n",
       "      101  11  ...    younger brother  youth  zane  zany  zero  zeta  \\\n",
       "0       0   0  ...                  0      0     0     0     0     0   \n",
       "1       0   0  ...                  0      0     0     0     0     0   \n",
       "2       0   0  ...                  0      0     0     0     0     0   \n",
       "3       0   0  ...                  0      0     0     0     0     0   \n",
       "4       0   0  ...                  0      0     0     0     0     0   \n",
       "5       0   0  ...                  0      0     0     0     0     0   \n",
       "6       0   0  ...                  0      0     0     0     0     0   \n",
       "7       0   0  ...                  0      0     1     0     0     0   \n",
       "8       0   0  ...                  0      2     0     0     0     0   \n",
       "9       0   0  ...                  0      0     0     0     0     0   \n",
       "10      0   0  ...                  0      0     0     0     0     0   \n",
       "11      0   0  ...                  0      0     0     0     0     0   \n",
       "12      0   0  ...                  1      0     0     0     0     0   \n",
       "13      0   0  ...                  0      0     0     0     0     0   \n",
       "14      0   0  ...                  0      0     0     0     0     0   \n",
       "15      0   0  ...                  0      0     0     0     0     0   \n",
       "16      0   0  ...                  0      0     0     0     0     2   \n",
       "17      0   0  ...                  0      0     0     0     0     0   \n",
       "18      0   0  ...                  0      0     0     0     0     0   \n",
       "19      0   0  ...                  0      0     0     0     0     0   \n",
       "20      0   0  ...                  0      0     0     0     0     0   \n",
       "21      0   0  ...                  0      0     0     0     0     0   \n",
       "22      0   0  ...                  0      0     0     0     0     0   \n",
       "23      0   0  ...                  0      0     0     0     0     0   \n",
       "24      0   0  ...                  0      0     0     0     0     0   \n",
       "25      0   0  ...                  0      0     0     0     0     0   \n",
       "26      0   0  ...                  0      0     0     0     0     0   \n",
       "27      0   0  ...                  0      0     0     0     0     0   \n",
       "28      0   0  ...                  0      0     0     0     0     0   \n",
       "29      0   0  ...                  0      0     0     0     0     0   \n",
       "...   ...  ..  ...                ...    ...   ...   ...   ...   ...   \n",
       "1470    0   0  ...                  0      0     0     0     0     0   \n",
       "1471    0   0  ...                  0      0     0     0     0     0   \n",
       "1472    0   0  ...                  0      0     0     0     0     0   \n",
       "1473    1   0  ...                  0      0     0     0     0     0   \n",
       "1474    0   0  ...                  0      0     0     0     0     0   \n",
       "1475    0   0  ...                  0      0     0     0     0     0   \n",
       "1476    0   0  ...                  0      0     0     0     0     0   \n",
       "1477    0   0  ...                  0      0     0     0     0     0   \n",
       "1478    0   0  ...                  0      0     0     0     0     0   \n",
       "1479    0   0  ...                  0      0     0     0     0     0   \n",
       "1480    0   0  ...                  0      0     0     0     0     0   \n",
       "1481    0   0  ...                  0      0     0     0     0     0   \n",
       "1482    0   0  ...                  0      0     0     0     0     0   \n",
       "1483    0   0  ...                  0      0     0     0     0     0   \n",
       "1484    0   0  ...                  0      0     0     0     0     0   \n",
       "1485    0   0  ...                  0      0     0     0     0     0   \n",
       "1486    0   0  ...                  0      0     0     0     0     0   \n",
       "1487    0   0  ...                  0      0     0     0     0     0   \n",
       "1488    0   0  ...                  0      0     0     0     0     0   \n",
       "1489    0   0  ...                  0      0     0     0     0     0   \n",
       "1490    0   0  ...                  0      0     0     0     0     0   \n",
       "1491    0   0  ...                  0      0     0     0     0     0   \n",
       "1492    0   0  ...                  0      0     0     0     0     0   \n",
       "1493    0   0  ...                  0      0     0     0     1     0   \n",
       "1494    0   0  ...                  0      0     0     0     0     0   \n",
       "1495    0   0  ...                  0      0     0     0     1     0   \n",
       "1496    0   0  ...                  0      0     0     0     0     0   \n",
       "1497    0   0  ...                  0      0     0     0     0     0   \n",
       "1498    0   0  ...                  0      0     0     0     0     0   \n",
       "1499    0   0  ...                  0      0     0     0     0     0   \n",
       "\n",
       "      zeta jones  zombie  zone  zooms  \n",
       "0              0       0     0      0  \n",
       "1              0       0     0      0  \n",
       "2              0       0     0      0  \n",
       "3              0       0     0      0  \n",
       "4              0       0     0      0  \n",
       "5              0       0     0      0  \n",
       "6              0       0     0      0  \n",
       "7              0       0     0      0  \n",
       "8              0       0     0      0  \n",
       "9              0       0     0      0  \n",
       "10             0       0     0      0  \n",
       "11             0       0     0      0  \n",
       "12             0       0     0      0  \n",
       "13             0       0     0      0  \n",
       "14             0       0     0      0  \n",
       "15             0       0     0      0  \n",
       "16             2       0     0      1  \n",
       "17             0       0     0      0  \n",
       "18             0       0     0      0  \n",
       "19             0       0     0      0  \n",
       "20             0       0     0      0  \n",
       "21             0       0     0      0  \n",
       "22             0       0     0      0  \n",
       "23             0       0     0      0  \n",
       "24             0       0     0      0  \n",
       "25             0       0     0      0  \n",
       "26             0       0     0      0  \n",
       "27             0       0     0      0  \n",
       "28             0       0     0      1  \n",
       "29             0       0     0      0  \n",
       "...          ...     ...   ...    ...  \n",
       "1470           0       0     0      0  \n",
       "1471           0       0     0      0  \n",
       "1472           0       0     0      0  \n",
       "1473           0       0     0      0  \n",
       "1474           0       0     0      0  \n",
       "1475           0       0     0      0  \n",
       "1476           0       0     0      0  \n",
       "1477           0       0     0      0  \n",
       "1478           0       0     0      0  \n",
       "1479           0       0     0      0  \n",
       "1480           0       0     0      0  \n",
       "1481           0       0     0      0  \n",
       "1482           0       0     0      0  \n",
       "1483           0       0     0      0  \n",
       "1484           0       0     0      0  \n",
       "1485           0       0     0      0  \n",
       "1486           0       0     0      0  \n",
       "1487           0       0     0      0  \n",
       "1488           0       0     0      0  \n",
       "1489           0       0     0      0  \n",
       "1490           0       0     0      0  \n",
       "1491           0       0     0      0  \n",
       "1492           0       0     0      0  \n",
       "1493           0       0     0      0  \n",
       "1494           0       0     0      0  \n",
       "1495           0       0     0      0  \n",
       "1496           0       0     0      0  \n",
       "1497           0       0     0      0  \n",
       "1498           0       0     0      0  \n",
       "1499           0       0     0      0  \n",
       "\n",
       "[1500 rows x 7836 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the vocabulary and document-term matrix together (X_train_dtm.toarray() converts sparse matrix to a dense matrix)\n",
    "pd.DataFrame(X_train_dtm.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<500x7836 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 95631 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "X_test_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class prediction with Multinomial Naive Bayes\n",
    "The first classifier we test here is a classical one for textual data represented as bag of words, and the combination of these two was a reference until the explosion of deep learning. So it's a good start to have a feeling of the performance we can get on our dataset. The use is really simple with scikit-learn, since it stands in 3 lines for the training and 1 line for a prediction on test data. More, this method has the advantage to have really low computation time, and not to require lots of training data like deep learning methods. It has 3 parameters (from scikit-learn documentation):\n",
    " - alpha: additive (Laplace/Lidstone) smoothing parameter (default=1.0, 0 for no smoothing)\n",
    " - fit_prior: whether to learn class prior probabilities or not. If false, a uniform prior will be used (default=True)\n",
    " - class_prior: enables to give one's own prior probabilities to the classes. If specified the priors are not adjusted according to the data (default=None)\n",
    "\n",
    "Here we already know that the 2 classes are equaly distributed, so the fit_prior parameter has no impact: we will get the same results by computing prior probabilities or by imposing a uniform distribution. And we don't try other priors either as it would give worse results. However alpha has a real impact that we evaluate in the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scikit-learn library\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a Multinomial Naive Bayes model\n",
    "alpha = 2 #(default=1.0, 0 for no smoothing)\n",
    "MNB = MultinomialNB(alpha, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 80.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=2, class_prior=None, fit_prior=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model using X_train_dtm (timing it with an IPython \"magic command\")\n",
    "%time MNB.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class predictions for X_test_dtm\n",
    "y_pred_class = MNB.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation\n",
    "We evaluate each classifier with common metrics: accuracy and area under ROC curve, and also compute the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 0.808\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy of class predictions\n",
    "accuracy_MNB = metrics.accuracy_score(y_test, y_pred_class)\n",
    "print('accuracy is: {}'.format(accuracy_MNB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "area under ROC curve is: 0.884\n"
     ]
    }
   ],
   "source": [
    "# calculate predicted probabilities for X_test_dtm and AUC\n",
    "y_pred_prob = MNB.predict_proba(X_test_dtm)[:, 1]\n",
    "AUC_MNB = metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "print('area under ROC curve is: {}'.format(round(AUC_MNB,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix is:\n",
      "[[195  50]\n",
      " [ 46 209]]\n"
     ]
    }
   ],
   "source": [
    "# print the confusion matrix\n",
    "conf_mat_MNB = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "print('confusion matrix is:')\n",
    "print(conf_mat_MNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 50 false positive samples:\n",
      "211    the rich legacy of cinema has left us with cer...\n",
      "683    often similar to a little boy lost in a park t...\n",
      "822    it's difficult to expect much from a director ...\n",
      "927    star wars : ? episode i -- the phantom menace ...\n",
      "762    weighed down by tired plot lines and spielberg...\n",
      "481    as any reasonable human being would , i must a...\n",
      "87     lucas was wise to start his star wars trilogy ...\n",
      "315    since director steven zaillian previously wrot...\n",
      "238    there is a scene early in jakob the liar that ...\n",
      "541    whether or not i would be considered a trekker...\n",
      "244    i read the new yorker magazine and i enjoy som...\n",
      "110    nearly every film tim burton has directed has ...\n",
      "552    kate ( jennifer aniston ) is having some probl...\n",
      "168    so , it's thirty years later , and oscar and f...\n",
      "263    it's not a bad thing to update old stories . \\...\n",
      "603    by phil curtolo \" madonna - antonio banderas -...\n",
      "216    libby parsons ( ashley judd ) has the perfect ...\n",
      "786    for a film touted as exploring relationships a...\n",
      "268    i have always been a fan of director neil jord...\n",
      "635    i didn't come into city of angels expecting gr...\n",
      "529    here's a rarity : a children's film that attem...\n",
      "42     out of sight director steven sorderbergh baffl...\n",
      "377    robin williams has the rarest of gifts : the a...\n",
      "806    my first exposure to the nightmare on elm stre...\n",
      "905    it's a good thing most animated sci-fi movies ...\n",
      "640    i'll be the first to admit it . \\nwhen you men...\n",
      "736    this well-conceived but ultra sugary coming-of...\n",
      "647     \" with all that education , you should know w...\n",
      "199    this independent film written and directed by ...\n",
      "9      call it a road trip for the walking wounded . ...\n",
      "179    starring ben stiller , elizabeth hurley , mari...\n",
      "900    in life , eddie murphy and martin lawrence pla...\n",
      "380    in my review of \" the spy who shagged me , \" i...\n",
      "476    miramax \" disinvited \" on-line media from pres...\n",
      "302    carry on at your convenience is all about the ...\n",
      "652    the real blonde ( r ) a woman's face , an arm ...\n",
      "382    for a movie with such deep religious and spiri...\n",
      "976    stars : armand assante ( mike hammer ) , barba...\n",
      "82     coinciding with the emerging popularity of mov...\n",
      "853    delicatessen ( directors : marc caro/jean-pier...\n",
      "668     \" the red violin \" is a cold , sterile featur...\n",
      "676    have you ever been in an automobile accident w...\n",
      "632    the swooping shots across darkened rooftops su...\n",
      "672    david schwimmer ( from the television series \"...\n",
      "973    'bicentennial man' is a family film without an...\n",
      "992    the king and i , a warner brothers animated , ...\n",
      "804    note : some may consider portions of the follo...\n",
      "354     \" the 44 caliber killer has struck again . \" ...\n",
      "91     various films seen at the seattle film festiva...\n",
      "530    there's no reason to doubt that donnie brasco ...\n",
      "Name: content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print reviews for the false positives\n",
    "FP_MNB = X_test[(y_pred_class==1) & (y_test==-1)]\n",
    "print('there are {} false positive samples:'.format(len(FP_MNB)))\n",
    "print(FP_MNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 46 false negative samples:\n",
      "859    mike myers , you certainly did throw us a ? fr...\n",
      "882    the word 'rest' in the title should be stresse...\n",
      "283    ok , let's get one thing straight right away :...\n",
      "694    let me first say that the conditions that i wa...\n",
      "656    jacques tati's 1953 classic \" les vacances de ...\n",
      "127    i have to say it . \\ntim burton's retelling of...\n",
      "237    jerry springer has got nothing on \" wild thing...\n",
      "373    okay , let me first say , this is a beavis and...\n",
      "995    a thriller set in modern day seattle , that ma...\n",
      "987    i think the first thing this reviewer should m...\n",
      "701    let me start off by saying that leading up to ...\n",
      "556    harmless , silly and fun comedy about dim-witt...\n",
      "762    film adaptation of hunter s . thompson's infam...\n",
      "349    bob the happy bastard's quickie review : \\nthe...\n",
      "481    the trailers and the beginning of the move sum...\n",
      "511    are we victims of fate in life or can we creat...\n",
      "390    when i left the theater after seeing david lyn...\n",
      "988    trees lounge is the directoral debut from one ...\n",
      "195    it's a fact that a good thriller or action mov...\n",
      "76     well , i know that stallone is 50 years old no...\n",
      "293    the muppet movie is the first , and the best m...\n",
      "120    disaster films have a tendency to be very form...\n",
      "862    an indian runner was more than a courier . \\nh...\n",
      "953    director dominic sena ( who made the highly un...\n",
      "224    because the press screening of \" planet of the...\n",
      "508    capsule : side-splitting comedy that follows i...\n",
      "397    well i'll be damned , what a most excellent su...\n",
      "83     a big surprise to me . \\nthe good trailer had ...\n",
      "105    based on the boris karloff's classic by the sa...\n",
      "433    based on the relatively unknown ( in compariso...\n",
      "404    my fellow americans is a movie that at first g...\n",
      "108    is evil dead ii a bad movie ? \\nit's full of t...\n",
      "703    the characters in \" palmetto \" collectively sw...\n",
      "260    richard linklater's \" slacker , \" made in 1991...\n",
      "50     warning : contains what the matrix is . \\nrate...\n",
      "401    the happy bastard's 30-second review : \\nameri...\n",
      "1      films adapted from comic books have had plenty...\n",
      "817    in my review of there's something about mary ,...\n",
      "791    tempe mills cinema , az--this movie had us in ...\n",
      "306    of all the films i've come to see this year ( ...\n",
      "992    i don't box with kid gloves . \\ni don't play n...\n",
      "81     just how inseparable is the team of sgt . \\nma...\n",
      "608    us critic-type people are always shaking our h...\n",
      "465    swashbuckling adventure that can be enjoyed by...\n",
      "447     \" desperate measures \" was something i was ex...\n",
      "41      \" he's back , and it's about time . \" \\nwas t...\n",
      "Name: content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print reviews for the false negatives\n",
    "FN_MNB = X_test[(y_pred_class==-1) & (y_test==1)]\n",
    "print('there are {} false negative samples:'.format(len(FN_MNB)))\n",
    "print(FN_MNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning with cross-validation\n",
    "We evaluate the impact of the parameter alpha and try to find the best value with grid search cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameters\n",
    "alpha_range = [1.0e-10, 0.25, 0.5, 1, 1.5, 2, 3, 4, 6, 8] #0 not accepted in MultinomialNB and automatically mapped to 1.0e-10\n",
    "param_grid = dict(alpha=alpha_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'alpha': [1e-10, 0.25, 0.5, 1, 1.5, 2, 3, 4, 6, 8]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# operate grid search with default RBF kernel\n",
    "grid = GridSearchCV(MultinomialNB(), param_grid=param_grid, cv=3, return_train_score=True)\n",
    "grid.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best alpha is {'alpha': 2} with a score of 0.81\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.68733333, 0.78333333, 0.79066667, 0.796     , 0.80733333,\n",
       "       0.81266667, 0.806     , 0.786     , 0.75866667, 0.72333333])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print results\n",
    "print(\"The best alpha is %s with a score of %0.2f\" % (grid.best_params_, grid.best_score_))\n",
    "scores = grid.cv_results_['mean_test_score']\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot scores\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(alpha_range, scores)\n",
    "plt.xlabel('alpha')\n",
    "plt.title('Multinomial Naive Bayes: validation accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class prediction with logistic regression\n",
    "A second common classification method is logistic regression, which is also provided by scikit-learn. The main parameters are (from scikit-learn documentation):\n",
    " - solver: algorithm to use in the optimization problem, among newton-cg, lbfgs, liblinear, sag, saga (default: liblinear, but will be changed to 'lbfgs' in scikit-learn v0.22)\n",
    " - max_iter: maximum number of iterations taken for the solvers newton-cg, sag and lbfgs to converge\n",
    " - penalty: norm used in the penalization (l1 or l2, default: l2).\n",
    " - dual: use of dual or primal formulation (default: False). Dual formulation is only implemented for l2 penalty.\n",
    " - tol: tolerance for stopping criteria (default: 1e-4)\n",
    " - C: inverse of regularization strength (default: 1.0). Like in SVMs, smaller values specify stronger regularization.\n",
    " - fit_intercept: whether a constant (a.k.a. bias or intercept) should be added to the decision function (default: True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and instantiate a logistic regression model \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 705 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model using X_train_dtm\n",
    "%time logreg.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class predictions for X_test_dtm\n",
    "y_pred_class = logreg.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 0.842\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "accuracy_LogReg = metrics.accuracy_score(y_test, y_pred_class)\n",
    "print('accuracy is: {}'.format(accuracy_LogReg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "area under ROC curve is: 0.912\n"
     ]
    }
   ],
   "source": [
    "# calculate predicted probabilities for X_test_dtm and area under ROC curve\n",
    "y_pred_prob = logreg.predict_proba(X_test_dtm)[:, 1]\n",
    "AUC_LogReg = metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "print('area under ROC curve is: {}'.format(round(AUC_LogReg,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix is:\n",
      "[[211  34]\n",
      " [ 45 210]]\n"
     ]
    }
   ],
   "source": [
    "# print the confusion matrix\n",
    "conf_mat_LogReg = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "print('confusion matrix is:')\n",
    "print(conf_mat_LogReg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with another solver\n",
    "Default solver will be changed to 'lbfgs' in scikit-learn v0.22. So it may be useful to try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a logistic regression model \n",
    "logreg = LogisticRegression(solver='lbfgs', max_iter=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.97 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=300, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model using X_train_dtm\n",
    "%time logreg.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class predictions for X_test_dtm\n",
    "y_pred_class = logreg.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 0.836\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "accuracy_LogReg = metrics.accuracy_score(y_test, y_pred_class)\n",
    "print('accuracy is: {}'.format(accuracy_LogReg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "area under ROC curve is: 0.912\n"
     ]
    }
   ],
   "source": [
    "# calculate predicted probabilities for X_test_dtm and area under ROC curve\n",
    "y_pred_prob = logreg.predict_proba(X_test_dtm)[:, 1]\n",
    "AUC_LogReg = metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "print('area under ROC curve is: {}'.format(round(AUC_LogReg,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix is:\n",
      "[[210  35]\n",
      " [ 47 208]]\n"
     ]
    }
   ],
   "source": [
    "# print the confusion matrix\n",
    "conf_mat_LogReg = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "print('confusion matrix is:')\n",
    "print(conf_mat_LogReg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning parameters with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_range = np.logspace(0, 10, 6)\n",
    "gamma_range = np.logspace(-9, -1, 5)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# operate grid search\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=3, return_train_score=True)\n",
    "grid.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "print(\"The best parameters are %s with a score of %0.2f\" % (grid.best_params_, grid.best_score_))\n",
    "scores = grid.cv_results_['mean_test_score'].reshape(len(C_range),len(gamma_range))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw heatmap\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot)\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('C')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)\n",
    "plt.yticks(np.arange(len(C_range)), C_range)\n",
    "plt.title('Validation accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class prediction with SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import, instantiate and train a SVM model without probability estimation\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='linear')\n",
    "%time clf.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class predictions for X_test_dtm\n",
    "y_pred_class = clf.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the confusion matrix\n",
    "metrics.confusion_matrix(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import, instantiate and train a SVM model with probability estimation\n",
    "clf = SVC(kernel='linear', probability=True)\n",
    "%time clf.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate predicted probabilities for X_test_dtm\n",
    "y_pred_prob = clf.predict_proba(X_test_dtm)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate AUC\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning parameters with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st step: large logarithmic grid search\n",
    "C_range = np.logspace(0, 10, 6)\n",
    "gamma_range = np.logspace(-9, -1, 5)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# operate grid search with default RBF kernel\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=3, return_train_score=True)\n",
    "grid.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "print(\"The best parameters are %s with a score of %0.2f\" % (grid.best_params_, grid.best_score_))\n",
    "scores = grid.cv_results_['mean_test_score'].reshape(len(C_range),len(gamma_range))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw heatmap\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot)\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('C')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)\n",
    "plt.yticks(np.arange(len(C_range)), C_range)\n",
    "plt.title('Validation accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd step: precise logarithmic grid search on selected range\n",
    "C_range = np.logspace(4, 8, 5)\n",
    "gamma_range = np.logspace(-8, -4, 5)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "grid = GridSearchCV(SVC(kernel='linear'), param_grid=param_grid)\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# operate grid search with default RBF kernel\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=3, return_train_score=True)\n",
    "grid.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "print(\"The best parameters are %s with a score of %0.2f\" % (grid.best_params_, grid.best_score_))\n",
    "scores = grid.cv_results_['mean_test_score'].reshape(len(C_range),len(gamma_range))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot)\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('C')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)\n",
    "plt.yticks(np.arange(len(C_range)), C_range)\n",
    "plt.title('Validation accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class prediction using bidirectional LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another method of classification, we will use a bidirectional LSTM (<a href=https://maxwell.ict.griffith.edu.au/spl/publications/papers/ieeesp97_schuster.pdf>Schuster and al, 1997</a>). <br>\n",
    "\n",
    "The difference with a simple LSTM is that it agregate the result of a first LSTM whose input is gathered <br>\n",
    "from left to right with another LSTM whose input is gathered from right to left. <br>\n",
    "For this purpose, we will use of KERAS, a framework which make use of tensorflow (or theano) for python3. <br>\n",
    "\n",
    "The purpose of a bidirectional LSTM is to outperform the performance of a simple LSTM.\n",
    "\n",
    "In order to fulfil the motivations of reproducible research, both tensorflow (used by KERAS) and numpy seeds are explicitly set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "#Seeds of KERAS and numpy frozen here\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to bypass out of memory error on GPU: we run this session on CPU only\n",
    "#If the gpu has a large enough amount of memory, the content of this cell can be discarded\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto(\n",
    "        device_count = {'GPU': 0}\n",
    "    )\n",
    "sess = set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      " - 2s - loss: 0.7185\n",
      "Epoch 2/250\n",
      " - 1s - loss: 0.3632\n",
      "Epoch 3/250\n",
      " - 1s - loss: 0.1833\n",
      "Epoch 4/250\n",
      " - 1s - loss: 0.0899\n",
      "Epoch 5/250\n",
      " - 1s - loss: 0.0144\n",
      "Epoch 6/250\n",
      " - 1s - loss: -4.9479e-02\n",
      "Epoch 7/250\n",
      " - 1s - loss: -1.0706e-01\n",
      "Epoch 8/250\n",
      " - 1s - loss: -1.6506e-01\n",
      "Epoch 9/250\n",
      " - 1s - loss: -2.2271e-01\n",
      "Epoch 10/250\n",
      " - 1s - loss: -2.7968e-01\n",
      "Epoch 11/250\n",
      " - 1s - loss: -3.3709e-01\n",
      "Epoch 12/250\n",
      " - 1s - loss: -3.9467e-01\n",
      "Epoch 13/250\n",
      " - 1s - loss: -4.5232e-01\n",
      "Epoch 14/250\n",
      " - 1s - loss: -5.0966e-01\n",
      "Epoch 15/250\n",
      " - 1s - loss: -5.6698e-01\n",
      "Epoch 16/250\n",
      " - 1s - loss: -6.2411e-01\n",
      "Epoch 17/250\n",
      " - 1s - loss: -6.8064e-01\n",
      "Epoch 18/250\n",
      " - 1s - loss: -7.3726e-01\n",
      "Epoch 19/250\n",
      " - 1s - loss: -7.9341e-01\n",
      "Epoch 20/250\n",
      " - 1s - loss: -8.4906e-01\n",
      "Epoch 21/250\n",
      " - 1s - loss: -9.0364e-01\n",
      "Epoch 22/250\n",
      " - 1s - loss: -9.5754e-01\n",
      "Epoch 23/250\n",
      " - 1s - loss: -1.0106e+00\n",
      "Epoch 24/250\n",
      " - 2s - loss: -1.0628e+00\n",
      "Epoch 25/250\n",
      " - 1s - loss: -1.1138e+00\n",
      "Epoch 26/250\n",
      " - 2s - loss: -1.1635e+00\n",
      "Epoch 27/250\n",
      " - 1s - loss: -1.2125e+00\n",
      "Epoch 28/250\n",
      " - 1s - loss: -1.2598e+00\n",
      "Epoch 29/250\n",
      " - 1s - loss: -1.3061e+00\n",
      "Epoch 30/250\n",
      " - 1s - loss: -1.3515e+00\n",
      "Epoch 31/250\n",
      " - 1s - loss: -1.3960e+00\n",
      "Epoch 32/250\n",
      " - 1s - loss: -1.4397e+00\n",
      "Epoch 33/250\n",
      " - 1s - loss: -1.4827e+00\n",
      "Epoch 34/250\n",
      " - 1s - loss: -1.5255e+00\n",
      "Epoch 35/250\n",
      " - 1s - loss: -1.5678e+00\n",
      "Epoch 36/250\n",
      " - 1s - loss: -1.6088e+00\n",
      "Epoch 37/250\n",
      " - 1s - loss: -1.6492e+00\n",
      "Epoch 38/250\n",
      " - 1s - loss: -1.6893e+00\n",
      "Epoch 39/250\n",
      " - 1s - loss: -1.7283e+00\n",
      "Epoch 40/250\n",
      " - 1s - loss: -1.7667e+00\n",
      "Epoch 41/250\n",
      " - 1s - loss: -1.8043e+00\n",
      "Epoch 42/250\n",
      " - 1s - loss: -1.8412e+00\n",
      "Epoch 43/250\n",
      " - 1s - loss: -1.8776e+00\n",
      "Epoch 44/250\n",
      " - 1s - loss: -1.9135e+00\n",
      "Epoch 45/250\n",
      " - 1s - loss: -1.9488e+00\n",
      "Epoch 46/250\n",
      " - 1s - loss: -1.9838e+00\n",
      "Epoch 47/250\n",
      " - 1s - loss: -2.0181e+00\n",
      "Epoch 48/250\n",
      " - 1s - loss: -2.0522e+00\n",
      "Epoch 49/250\n",
      " - 1s - loss: -2.0856e+00\n",
      "Epoch 50/250\n",
      " - 1s - loss: -2.1189e+00\n",
      "Epoch 51/250\n",
      " - 1s - loss: -2.1515e+00\n",
      "Epoch 52/250\n",
      " - 1s - loss: -2.1838e+00\n",
      "Epoch 53/250\n",
      " - 1s - loss: -2.2158e+00\n",
      "Epoch 54/250\n",
      " - 1s - loss: -2.2473e+00\n",
      "Epoch 55/250\n",
      " - 1s - loss: -2.2786e+00\n",
      "Epoch 56/250\n",
      " - 1s - loss: -2.3095e+00\n",
      "Epoch 57/250\n",
      " - 1s - loss: -2.3403e+00\n",
      "Epoch 58/250\n",
      " - 1s - loss: -2.3708e+00\n",
      "Epoch 59/250\n",
      " - 1s - loss: -2.4007e+00\n",
      "Epoch 60/250\n",
      " - 1s - loss: -2.4303e+00\n",
      "Epoch 61/250\n",
      " - 1s - loss: -2.4596e+00\n",
      "Epoch 62/250\n",
      " - 1s - loss: -2.4885e+00\n",
      "Epoch 63/250\n",
      " - 1s - loss: -2.5171e+00\n",
      "Epoch 64/250\n",
      " - 1s - loss: -2.5456e+00\n",
      "Epoch 65/250\n",
      " - 1s - loss: -2.5736e+00\n",
      "Epoch 66/250\n",
      " - 1s - loss: -2.6013e+00\n",
      "Epoch 67/250\n",
      " - 1s - loss: -2.6287e+00\n",
      "Epoch 68/250\n",
      " - 1s - loss: -2.6561e+00\n",
      "Epoch 69/250\n",
      " - 1s - loss: -2.6829e+00\n",
      "Epoch 70/250\n",
      " - 1s - loss: -2.7094e+00\n",
      "Epoch 71/250\n",
      " - 1s - loss: -2.7359e+00\n",
      "Epoch 72/250\n",
      " - 1s - loss: -2.7620e+00\n",
      "Epoch 73/250\n",
      " - 1s - loss: -2.7878e+00\n",
      "Epoch 74/250\n",
      " - 1s - loss: -2.8136e+00\n",
      "Epoch 75/250\n",
      " - 1s - loss: -2.8392e+00\n",
      "Epoch 76/250\n",
      " - 1s - loss: -2.8645e+00\n",
      "Epoch 77/250\n",
      " - 1s - loss: -2.8895e+00\n",
      "Epoch 78/250\n",
      " - 1s - loss: -2.9141e+00\n",
      "Epoch 79/250\n",
      " - 1s - loss: -2.9386e+00\n",
      "Epoch 80/250\n",
      " - 1s - loss: -2.9629e+00\n",
      "Epoch 81/250\n",
      " - 1s - loss: -2.9870e+00\n",
      "Epoch 82/250\n",
      " - 2s - loss: -3.0110e+00\n",
      "Epoch 83/250\n",
      " - 1s - loss: -3.0347e+00\n",
      "Epoch 84/250\n",
      " - 1s - loss: -3.0581e+00\n",
      "Epoch 85/250\n",
      " - 1s - loss: -3.0814e+00\n",
      "Epoch 86/250\n",
      " - 1s - loss: -3.1046e+00\n",
      "Epoch 87/250\n",
      " - 2s - loss: -3.1275e+00\n",
      "Epoch 88/250\n",
      " - 2s - loss: -3.1502e+00\n",
      "Epoch 89/250\n",
      " - 1s - loss: -3.1728e+00\n",
      "Epoch 90/250\n",
      " - 1s - loss: -3.1951e+00\n",
      "Epoch 91/250\n",
      " - 1s - loss: -3.2174e+00\n",
      "Epoch 92/250\n",
      " - 1s - loss: -3.2394e+00\n",
      "Epoch 93/250\n",
      " - 1s - loss: -3.2614e+00\n",
      "Epoch 94/250\n",
      " - 1s - loss: -3.2831e+00\n",
      "Epoch 95/250\n",
      " - 1s - loss: -3.3046e+00\n",
      "Epoch 96/250\n",
      " - 1s - loss: -3.3259e+00\n",
      "Epoch 97/250\n",
      " - 1s - loss: -3.3471e+00\n",
      "Epoch 98/250\n",
      " - 1s - loss: -3.3680e+00\n",
      "Epoch 99/250\n",
      " - 1s - loss: -3.3889e+00\n",
      "Epoch 100/250\n",
      " - 1s - loss: -3.4095e+00\n",
      "Epoch 101/250\n",
      " - 1s - loss: -3.4300e+00\n",
      "Epoch 102/250\n",
      " - 1s - loss: -3.4503e+00\n",
      "Epoch 103/250\n",
      " - 1s - loss: -3.4704e+00\n",
      "Epoch 104/250\n",
      " - 1s - loss: -3.4904e+00\n",
      "Epoch 105/250\n",
      " - 1s - loss: -3.5102e+00\n",
      "Epoch 106/250\n",
      " - 1s - loss: -3.5299e+00\n",
      "Epoch 107/250\n",
      " - 1s - loss: -3.5494e+00\n",
      "Epoch 108/250\n",
      " - 1s - loss: -3.5688e+00\n",
      "Epoch 109/250\n",
      " - 1s - loss: -3.5879e+00\n",
      "Epoch 110/250\n",
      " - 1s - loss: -3.6069e+00\n",
      "Epoch 111/250\n",
      " - 1s - loss: -3.6258e+00\n",
      "Epoch 112/250\n",
      " - 1s - loss: -3.6445e+00\n",
      "Epoch 113/250\n",
      " - 1s - loss: -3.6631e+00\n",
      "Epoch 114/250\n",
      " - 1s - loss: -3.6815e+00\n",
      "Epoch 115/250\n",
      " - 1s - loss: -3.6998e+00\n",
      "Epoch 116/250\n",
      " - 1s - loss: -3.7179e+00\n",
      "Epoch 117/250\n",
      " - 1s - loss: -3.7360e+00\n",
      "Epoch 118/250\n",
      " - 1s - loss: -3.7539e+00\n",
      "Epoch 119/250\n",
      " - 1s - loss: -3.7716e+00\n",
      "Epoch 120/250\n",
      " - 1s - loss: -3.7893e+00\n",
      "Epoch 121/250\n",
      " - 1s - loss: -3.8067e+00\n",
      "Epoch 122/250\n",
      " - 1s - loss: -3.8241e+00\n",
      "Epoch 123/250\n",
      " - 1s - loss: -3.8414e+00\n",
      "Epoch 124/250\n",
      " - 1s - loss: -3.8586e+00\n",
      "Epoch 125/250\n",
      " - 1s - loss: -3.8756e+00\n",
      "Epoch 126/250\n",
      " - 1s - loss: -3.8925e+00\n",
      "Epoch 127/250\n",
      " - 1s - loss: -3.9094e+00\n",
      "Epoch 128/250\n",
      " - 1s - loss: -3.9261e+00\n",
      "Epoch 129/250\n",
      " - 1s - loss: -3.9427e+00\n",
      "Epoch 130/250\n",
      " - 1s - loss: -3.9593e+00\n",
      "Epoch 131/250\n",
      " - 1s - loss: -3.9757e+00\n",
      "Epoch 132/250\n",
      " - 1s - loss: -3.9920e+00\n",
      "Epoch 133/250\n",
      " - 1s - loss: -4.0083e+00\n",
      "Epoch 134/250\n",
      " - 2s - loss: -4.0244e+00\n",
      "Epoch 135/250\n",
      " - 1s - loss: -4.0404e+00\n",
      "Epoch 136/250\n",
      " - 2s - loss: -4.0564e+00\n",
      "Epoch 137/250\n",
      " - 2s - loss: -4.0722e+00\n",
      "Epoch 138/250\n",
      " - 1s - loss: -4.0880e+00\n",
      "Epoch 139/250\n",
      " - 1s - loss: -4.1037e+00\n",
      "Epoch 140/250\n",
      " - 1s - loss: -4.1194e+00\n",
      "Epoch 141/250\n",
      " - 1s - loss: -4.1349e+00\n",
      "Epoch 142/250\n",
      " - 1s - loss: -4.1504e+00\n",
      "Epoch 143/250\n",
      " - 1s - loss: -4.1658e+00\n",
      "Epoch 144/250\n",
      " - 1s - loss: -4.1811e+00\n",
      "Epoch 145/250\n",
      " - 1s - loss: -4.1964e+00\n",
      "Epoch 146/250\n",
      " - 1s - loss: -4.2116e+00\n",
      "Epoch 147/250\n",
      " - 1s - loss: -4.2267e+00\n",
      "Epoch 148/250\n",
      " - 1s - loss: -4.2417e+00\n",
      "Epoch 149/250\n",
      " - 1s - loss: -4.2566e+00\n",
      "Epoch 150/250\n",
      " - 1s - loss: -4.2715e+00\n",
      "Epoch 151/250\n",
      " - 1s - loss: -4.2863e+00\n",
      "Epoch 152/250\n",
      " - 1s - loss: -4.3011e+00\n",
      "Epoch 153/250\n",
      " - 1s - loss: -4.3158e+00\n",
      "Epoch 154/250\n",
      " - 1s - loss: -4.3304e+00\n",
      "Epoch 155/250\n",
      " - 1s - loss: -4.3449e+00\n",
      "Epoch 156/250\n",
      " - 1s - loss: -4.3594e+00\n",
      "Epoch 157/250\n",
      " - 1s - loss: -4.3738e+00\n",
      "Epoch 158/250\n",
      " - 1s - loss: -4.3882e+00\n",
      "Epoch 159/250\n",
      " - 1s - loss: -4.4025e+00\n",
      "Epoch 160/250\n",
      " - 1s - loss: -4.4168e+00\n",
      "Epoch 161/250\n",
      " - 1s - loss: -4.4311e+00\n",
      "Epoch 162/250\n",
      " - 1s - loss: -4.4453e+00\n",
      "Epoch 163/250\n",
      " - 1s - loss: -4.4594e+00\n",
      "Epoch 164/250\n",
      " - 1s - loss: -4.4735e+00\n",
      "Epoch 165/250\n",
      " - 1s - loss: -4.4875e+00\n",
      "Epoch 166/250\n",
      " - 1s - loss: -4.5015e+00\n",
      "Epoch 167/250\n",
      " - 1s - loss: -4.5154e+00\n",
      "Epoch 168/250\n",
      " - 2s - loss: -4.5292e+00\n",
      "Epoch 169/250\n",
      " - 1s - loss: -4.5430e+00\n",
      "Epoch 170/250\n",
      " - 1s - loss: -4.5568e+00\n",
      "Epoch 171/250\n",
      " - 1s - loss: -4.5705e+00\n",
      "Epoch 172/250\n",
      " - 1s - loss: -4.5842e+00\n",
      "Epoch 173/250\n",
      " - 1s - loss: -4.5978e+00\n",
      "Epoch 174/250\n",
      " - 1s - loss: -4.6115e+00\n",
      "Epoch 175/250\n",
      " - 1s - loss: -4.6250e+00\n",
      "Epoch 176/250\n",
      " - 2s - loss: -4.6385e+00\n",
      "Epoch 177/250\n",
      " - 1s - loss: -4.6520e+00\n",
      "Epoch 178/250\n",
      " - 1s - loss: -4.6654e+00\n",
      "Epoch 179/250\n",
      " - 1s - loss: -4.6788e+00\n",
      "Epoch 180/250\n",
      " - 1s - loss: -4.6921e+00\n",
      "Epoch 181/250\n",
      " - 1s - loss: -4.7054e+00\n",
      "Epoch 182/250\n",
      " - 2s - loss: -4.7186e+00\n",
      "Epoch 183/250\n",
      " - 1s - loss: -4.7318e+00\n",
      "Epoch 184/250\n",
      " - 1s - loss: -4.7450e+00\n",
      "Epoch 185/250\n",
      " - 1s - loss: -4.7581e+00\n",
      "Epoch 186/250\n",
      " - 1s - loss: -4.7712e+00\n",
      "Epoch 187/250\n",
      " - 1s - loss: -4.7843e+00\n",
      "Epoch 188/250\n",
      " - 1s - loss: -4.7974e+00\n",
      "Epoch 189/250\n",
      " - 2s - loss: -4.8104e+00\n",
      "Epoch 190/250\n",
      " - 2s - loss: -4.8233e+00\n",
      "Epoch 191/250\n",
      " - 2s - loss: -4.8363e+00\n",
      "Epoch 192/250\n",
      " - 1s - loss: -4.8492e+00\n",
      "Epoch 193/250\n",
      " - 1s - loss: -4.8621e+00\n",
      "Epoch 194/250\n",
      " - 1s - loss: -4.8750e+00\n",
      "Epoch 195/250\n",
      " - 1s - loss: -4.8878e+00\n",
      "Epoch 196/250\n",
      " - 2s - loss: -4.9007e+00\n",
      "Epoch 197/250\n",
      " - 2s - loss: -4.9135e+00\n",
      "Epoch 198/250\n",
      " - 1s - loss: -4.9262e+00\n",
      "Epoch 199/250\n",
      " - 1s - loss: -4.9390e+00\n",
      "Epoch 200/250\n",
      " - 1s - loss: -4.9517e+00\n",
      "Epoch 201/250\n",
      " - 1s - loss: -4.9644e+00\n",
      "Epoch 202/250\n",
      " - 1s - loss: -4.9770e+00\n",
      "Epoch 203/250\n",
      " - 1s - loss: -4.9897e+00\n",
      "Epoch 204/250\n",
      " - 1s - loss: -5.0024e+00\n",
      "Epoch 205/250\n",
      " - 1s - loss: -5.0150e+00\n",
      "Epoch 206/250\n",
      " - 1s - loss: -5.0276e+00\n",
      "Epoch 207/250\n",
      " - 1s - loss: -5.0401e+00\n",
      "Epoch 208/250\n",
      " - 1s - loss: -5.0527e+00\n",
      "Epoch 209/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: -5.0653e+00\n",
      "Epoch 210/250\n",
      " - 1s - loss: -5.0778e+00\n",
      "Epoch 211/250\n",
      " - 2s - loss: -5.0903e+00\n",
      "Epoch 212/250\n",
      " - 1s - loss: -5.1028e+00\n",
      "Epoch 213/250\n",
      " - 2s - loss: -5.1153e+00\n",
      "Epoch 214/250\n",
      " - 2s - loss: -5.1277e+00\n",
      "Epoch 215/250\n",
      " - 2s - loss: -5.1402e+00\n",
      "Epoch 216/250\n",
      " - 1s - loss: -5.1526e+00\n",
      "Epoch 217/250\n",
      " - 2s - loss: -5.1650e+00\n",
      "Epoch 218/250\n",
      " - 1s - loss: -5.1774e+00\n",
      "Epoch 219/250\n",
      " - 2s - loss: -5.1897e+00\n",
      "Epoch 220/250\n",
      " - 2s - loss: -5.2021e+00\n",
      "Epoch 221/250\n",
      " - 2s - loss: -5.2144e+00\n",
      "Epoch 222/250\n",
      " - 1s - loss: -5.2267e+00\n",
      "Epoch 223/250\n",
      " - 1s - loss: -5.2390e+00\n",
      "Epoch 224/250\n",
      " - 1s - loss: -5.2513e+00\n",
      "Epoch 225/250\n",
      " - 1s - loss: -5.2636e+00\n",
      "Epoch 226/250\n",
      " - 2s - loss: -5.2758e+00\n",
      "Epoch 227/250\n",
      " - 2s - loss: -5.2881e+00\n",
      "Epoch 228/250\n",
      " - 1s - loss: -5.3003e+00\n",
      "Epoch 229/250\n",
      " - 1s - loss: -5.3126e+00\n",
      "Epoch 230/250\n",
      " - 1s - loss: -5.3248e+00\n",
      "Epoch 231/250\n",
      " - 2s - loss: -5.3370e+00\n",
      "Epoch 232/250\n",
      " - 2s - loss: -5.3492e+00\n",
      "Epoch 233/250\n",
      " - 2s - loss: -5.3614e+00\n",
      "Epoch 234/250\n",
      " - 2s - loss: -5.3735e+00\n",
      "Epoch 235/250\n",
      " - 2s - loss: -5.3857e+00\n",
      "Epoch 236/250\n",
      " - 2s - loss: -5.3978e+00\n",
      "Epoch 237/250\n",
      " - 2s - loss: -5.4099e+00\n",
      "Epoch 238/250\n",
      " - 2s - loss: -5.4221e+00\n",
      "Epoch 239/250\n",
      " - 2s - loss: -5.4342e+00\n",
      "Epoch 240/250\n",
      " - 2s - loss: -5.4462e+00\n",
      "Epoch 241/250\n",
      " - 2s - loss: -5.4583e+00\n",
      "Epoch 242/250\n",
      " - 2s - loss: -5.4704e+00\n",
      "Epoch 243/250\n",
      " - 2s - loss: -5.4825e+00\n",
      "Epoch 244/250\n",
      " - 1s - loss: -5.4945e+00\n",
      "Epoch 245/250\n",
      " - 1s - loss: -5.5066e+00\n",
      "Epoch 246/250\n",
      " - 1s - loss: -5.5186e+00\n",
      "Epoch 247/250\n",
      " - 1s - loss: -5.5306e+00\n",
      "Epoch 248/250\n",
      " - 2s - loss: -5.5426e+00\n",
      "Epoch 249/250\n",
      " - 2s - loss: -5.5546e+00\n",
      "Epoch 250/250\n",
      " - 2s - loss: -5.5666e+00\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected bidirectional_21_input to have shape (1500, 7836) but got array with shape (500, 7836)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-f9404d78d43c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m#Accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_ts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;31m#, y_ts, batch_size=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected bidirectional_21_input to have shape (1500, 7836) but got array with shape (500, 7836)"
     ]
    }
   ],
   "source": [
    "#We reshape the arrays for the lstm\n",
    "y_tr = y_train.values.reshape(1, y_train.shape[0], 1)\n",
    "X_tr = X_train_dtm.toarray().reshape(1, X_train_dtm.shape[0], X_train_dtm.shape[1])\n",
    "y_ts = y_test.values.reshape(1, y_test.shape[0], 1)\n",
    "X_ts = X_test_dtm.toarray().reshape(1, X_test_dtm.shape[0], X_test_dtm.shape[1])\n",
    "\n",
    "#In a terminal, we can launch a tool called tensorboard wich monitor the loss diminution and other variables.\n",
    "#    tensorboard --logdir path_to_current_folder/Graph \n",
    "\n",
    "#We create a tensorflow callback in order for tensorboard to vizualise the diminishing loss\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "#We create the KERAS model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(10, return_sequences=True), input_shape=X_tr.shape[1:], merge_mode='concat'))\n",
    "model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "#Training\n",
    "model.fit(X_tr, y_tr, epochs=25, batch_size=1, verbose=2, callbacks=[tbCallBack])\n",
    "\n",
    "#Accuracy #TOBECONTINUED <- shape erreur, corrig avant 16h\n",
    "#ypred = model.predict(X_ts)\n",
    "\n",
    "#from sklearn.metrics import accuracy_score\n",
    "#print(\"Accuracy: {}\".format(accuracy_score(y_ts, ypred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE HERE AND BELOW\n",
    "\n",
    "#We reshape the arrays for the lstm\n",
    "y_tr = y_train.values.reshape(1, y_train.shape[0], 1)\n",
    "X_tr = X_train_dtm.toarray().reshape(1, X_train_dtm.shape[0], X_train_dtm.shape[1])\n",
    "y_ts = y_test.values.reshape(1, y_test.shape[0], 1)\n",
    "X_ts = X_test_dtm.toarray().reshape(1, X_test_dtm.shape[0], X_test_dtm.shape[1])\n",
    "\n",
    "#In a terminal, we can launch a tool called tensorboard wich monitor the loss diminution and other variables.\n",
    "#    tensorboard --logdir path_to_current_folder/Graph \n",
    "\n",
    "#We create a tensorflow callback in order for tensorboard to vizualise the diminishing loss\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "#We create the KERAS model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(10, return_sequences=True), input_shape=X_tr.shape[1:], merge_mode='concat'))\n",
    "model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "#Training\n",
    "model.fit(X_tr, y_tr, epochs=25, batch_size=1, verbose=2, callbacks=[tbCallBack])\n",
    "    \n",
    "#Accuracy\n",
    "score = model.predict(X_ts)\n",
    "#, y_ts, batch_size=1)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1500, 1)\n",
      "(1, 1500, 7836)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.values.reshape(1, y_train.shape[0], 1).shape)\n",
    "print(X_train_dtm.toarray().reshape(1, X_train_dtm.shape[0], X_train_dtm.shape[1]).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
