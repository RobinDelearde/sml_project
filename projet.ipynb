{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning for sentiment classification on movie reviews\n",
    "This project aims at compare several classification algorithms and textual data representations with their parameters, on a dataset of movies reviews classified as positive or negative.\n",
    "We experiment a representation as token counts (CountVectorizer) and 4 classification algorithms:\n",
    " - Multinomial Naive Bayes\n",
    " - Logistic regression\n",
    " - SVM\n",
    " - bi-directionnal LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load common librairies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "First the data is transformed into pandas dataframes by some file manipulation. We use the Polarity dataset v2.0 of [Bo Pang and Lillian Lee, ACL 2004], which is available at: http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dedicated library\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing: prepare data\n",
    "col_names = ['content', 'label']\n",
    "pos = pd.DataFrame(columns = col_names)\n",
    "neg = pd.DataFrame(columns = col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add positive samples to the DataFrame structure\n",
    "i=1\n",
    "for fend in os.listdir('./dataset1/pos/'):\n",
    "    #data = pd.read_csv('./dataset1/pos/'+fend, sep = None, header = None)\n",
    "    file = open('./dataset1/pos/'+fend, 'r')\n",
    "    data = file.read()\n",
    "    #print(data)\n",
    "    file.close()\n",
    "    pos = pos.append(pd.DataFrame({'content':[data], 'label':int(1)}, index=[i]))\n",
    "    i+=1\n",
    "# add negative samples to the DataFrame structure\n",
    "i=1\n",
    "for fend in os.listdir('./dataset1/neg/'):\n",
    "    #data = pd.read_csv('./dataset1/neg/'+fend, sep = None, header = None)\n",
    "    file = open('./dataset1/neg/'+fend, 'r')\n",
    "    data = file.read()\n",
    "    file.close()\n",
    "    neg = neg.append(pd.DataFrame({'content':[data],'label':int(-1)}, index=[i]))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert label to a numerical variable\n",
    "#reviews[\"label_num\"] = reviews.label.map({\"1\":int(1), \"-1\":int(0)})\n",
    "pos[\"label_num\"] = pos['label'].astype(int)\n",
    "neg[\"label_num\"] = neg['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-processing done\n",
      "number of positive samples: 1000 \n",
      "number of negative samples: 1000 \n",
      "\n",
      "last positive samples:\n",
      "                                                content label  label_num\n",
      "996   wow ! what a movie . \\nit's everything a movie...     1          1\n",
      "997   richard gere can be a commanding actor , but h...     1          1\n",
      "998   glory--starring matthew broderick , denzel was...     1          1\n",
      "999   steven spielberg's second epic film on world w...     1          1\n",
      "1000  truman ( \" true-man \" ) burbank is the perfect...     1          1\n",
      "\n",
      "last negative samples:\n",
      "                                                content label  label_num\n",
      "996   if anything , \" stigmata \" should be taken as ...    -1         -1\n",
      "997   john boorman's \" zardoz \" is a goofy cinematic...    -1         -1\n",
      "998   the kids in the hall are an acquired taste . \\...    -1         -1\n",
      "999   there was a time when john carpenter was a gre...    -1         -1\n",
      "1000  two party guys bob their heads to haddaway's d...    -1         -1\n"
     ]
    }
   ],
   "source": [
    "# print\n",
    "print('pre-processing done')\n",
    "print('number of positive samples: {} '.format(len(pos)))\n",
    "print('number of negative samples: {} '.format(len(neg)))\n",
    "print()\n",
    "print('last positive samples:')\n",
    "print(pos.tail(5))\n",
    "print()\n",
    "print('last negative samples:')\n",
    "print(neg.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare learning datasets\n",
    "Then we mix the 2 dataframes of positive and negative samples into 2 random dataframes dedicated to training and testing. We verified that the random split was always done in the same way (with the same seed) to ensure that the results are the same every time we run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat positive and negative samples\n",
    "reviews = pos.append(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X (items) and y (labels)\n",
    "X = reviews.content\n",
    "y = reviews.label_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split randomly X and y into train and test sets (NB: always uses the same seed)\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train samples: 1500\n",
      "number of test samples: 500\n",
      "\n",
      "first train samples with their labels:\n",
      "651    james cmaeron's breakthrough feature was the f...\n",
      "105    this is not a simple plan about finding a plan...\n",
      "562    the seasoned capt . dudley smith ( james cromw...\n",
      "644    in `enemy at the gates' , jude law is a gifted...\n",
      "442    call me crazy , but i don't see saving private...\n",
      "629    writer/director lawrence kasdan had a hand in ...\n",
      "997    john boorman's \" zardoz \" is a goofy cinematic...\n",
      "681    it is an understood passion and an understood ...\n",
      "813    seen april 16 , 1999 at 10 p . m . at crossgat...\n",
      "505    american pie 2 is filled with laughs . \\nbut t...\n",
      "Name: content, dtype: object\n",
      "651    1\n",
      "105   -1\n",
      "562    1\n",
      "644    1\n",
      "442    1\n",
      "629   -1\n",
      "997   -1\n",
      "681    1\n",
      "813    1\n",
      "505   -1\n",
      "Name: label_num, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "# print\n",
    "print('number of train samples: {}'.format(len(X_train)))\n",
    "print('number of test samples: {}'.format(len(X_test)))\n",
    "print()\n",
    "print('first train samples with their labels:')\n",
    "print(X_train.head(10))\n",
    "print(y_train.head(10))\n",
    "#print()\n",
    "#print('first test samples with their labels:')\n",
    "#print(X_test.head(10))\n",
    "#print(y_test.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Representing text as numerical data (vectorization)\n",
    "In order to use text as input of the classification algorithms, we need to convert it into fixed-size numerical feature vectors. To do so we use scikit-learn CountVectorizer that \"converts text into a matrix of token counts\", or \"bag of words\". It first learns the vocabulary over the text given as input, and then transforms any textual data into a document-term matrix using the fitted vocabulary. We learn the vocabulary on the training dataset, and compute the document-term matrix representations for both training and testing data. The basic method uses the input token as such in the vocabulary, but it's also possible to use methods of natural language processing to use more elaborated tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and instantiate CountVectorizer (with the default parameters)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learn the \"vocabulary\" of the training data (occurs in-place)\n",
    "vect.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words in the vocabulary: 35152 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '0009f',\n",
       " '007',\n",
       " '00s',\n",
       " '03',\n",
       " '04',\n",
       " '05',\n",
       " '05425',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '10000',\n",
       " '100m',\n",
       " '101',\n",
       " '102',\n",
       " '103',\n",
       " '104',\n",
       " '105',\n",
       " '106',\n",
       " '107',\n",
       " '108',\n",
       " '109',\n",
       " '10s',\n",
       " '10th',\n",
       " '11',\n",
       " '110',\n",
       " '111',\n",
       " '112',\n",
       " '113',\n",
       " '1138',\n",
       " '114',\n",
       " '115',\n",
       " '117',\n",
       " '118',\n",
       " '11th',\n",
       " '12',\n",
       " '121',\n",
       " '122',\n",
       " '123',\n",
       " '125',\n",
       " '126',\n",
       " '127',\n",
       " '1272',\n",
       " '128',\n",
       " '129',\n",
       " '1298',\n",
       " '13',\n",
       " '130',\n",
       " '1305',\n",
       " '132',\n",
       " '133',\n",
       " '135',\n",
       " '137',\n",
       " '138',\n",
       " '13th',\n",
       " '14',\n",
       " '143',\n",
       " '14th',\n",
       " '15',\n",
       " '150',\n",
       " '1500s',\n",
       " '151',\n",
       " '152',\n",
       " '153',\n",
       " '155',\n",
       " '1583',\n",
       " '15th',\n",
       " '16',\n",
       " '160',\n",
       " '1600',\n",
       " '1600s',\n",
       " '161',\n",
       " '167',\n",
       " '1692',\n",
       " '16mm',\n",
       " '16x9',\n",
       " '17',\n",
       " '170',\n",
       " '1700s',\n",
       " '1709',\n",
       " '172',\n",
       " '175',\n",
       " '1773',\n",
       " '1791',\n",
       " '1792',\n",
       " '1793',\n",
       " '1794',\n",
       " '1799',\n",
       " '17th',\n",
       " '18',\n",
       " '180',\n",
       " '1800',\n",
       " '1800s',\n",
       " '1812',\n",
       " '1830s',\n",
       " '1839',\n",
       " '1847',\n",
       " '1862',\n",
       " '1865',\n",
       " '1869',\n",
       " '1871',\n",
       " '1885',\n",
       " '1888',\n",
       " '189',\n",
       " '1896',\n",
       " '1898',\n",
       " '18s',\n",
       " '18th',\n",
       " '19',\n",
       " '1900',\n",
       " '1900s',\n",
       " '1903',\n",
       " '1912',\n",
       " '1914',\n",
       " '1916',\n",
       " '1919',\n",
       " '1920s',\n",
       " '1922',\n",
       " '1923',\n",
       " '1925',\n",
       " '1926',\n",
       " '1928',\n",
       " '1930',\n",
       " '1930s',\n",
       " '1932',\n",
       " '1933',\n",
       " '1934',\n",
       " '1935',\n",
       " '1937',\n",
       " '1938',\n",
       " '1939',\n",
       " '1940',\n",
       " '1940s',\n",
       " '1941',\n",
       " '1943',\n",
       " '1944',\n",
       " '1945',\n",
       " '1947',\n",
       " '1948',\n",
       " '1949',\n",
       " '1950',\n",
       " '1950s',\n",
       " '1952',\n",
       " '1954',\n",
       " '1957',\n",
       " '1958',\n",
       " '1959',\n",
       " '1960',\n",
       " '1960s',\n",
       " '1961',\n",
       " '1962',\n",
       " '1963',\n",
       " '1964',\n",
       " '1965',\n",
       " '1966',\n",
       " '1967',\n",
       " '1968',\n",
       " '1969',\n",
       " '1970',\n",
       " '1970s',\n",
       " '1971',\n",
       " '1972',\n",
       " '1973',\n",
       " '1974',\n",
       " '1975',\n",
       " '1976',\n",
       " '1977',\n",
       " '1978',\n",
       " '1979',\n",
       " '1980',\n",
       " '1980s',\n",
       " '1981',\n",
       " '1982',\n",
       " '1983',\n",
       " '1984',\n",
       " '1985',\n",
       " '1986',\n",
       " '1987',\n",
       " '1988',\n",
       " '1989',\n",
       " '1990',\n",
       " '1990s',\n",
       " '1991',\n",
       " '1992',\n",
       " '1993',\n",
       " '1994',\n",
       " '1995',\n",
       " '1996',\n",
       " '1997',\n",
       " '1998',\n",
       " '1999',\n",
       " '19th',\n",
       " '1hr',\n",
       " '1st',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '2001',\n",
       " '2007',\n",
       " '2010',\n",
       " '2013',\n",
       " '2017',\n",
       " '2020',\n",
       " '2023',\n",
       " '2024',\n",
       " '2029',\n",
       " '2036',\n",
       " '2040',\n",
       " '2050',\n",
       " '2058',\n",
       " '206',\n",
       " '2065',\n",
       " '209',\n",
       " '2099',\n",
       " '20s',\n",
       " '20th',\n",
       " '20thcentury',\n",
       " '21',\n",
       " '2176',\n",
       " '21a',\n",
       " '21st',\n",
       " '22',\n",
       " '2293',\n",
       " '23',\n",
       " '234',\n",
       " '24',\n",
       " '2400',\n",
       " '2470',\n",
       " '24th',\n",
       " '25',\n",
       " '250',\n",
       " '25th',\n",
       " '26',\n",
       " '2654',\n",
       " '26th',\n",
       " '27',\n",
       " '28',\n",
       " '289',\n",
       " '28th',\n",
       " '28up',\n",
       " '29',\n",
       " '2_',\n",
       " '2am',\n",
       " '2d',\n",
       " '2hr',\n",
       " '2nd',\n",
       " '30',\n",
       " '300',\n",
       " '3000',\n",
       " '30ish',\n",
       " '30m',\n",
       " '30s',\n",
       " '30th',\n",
       " '31',\n",
       " '310',\n",
       " '32',\n",
       " '33',\n",
       " '34',\n",
       " '3411',\n",
       " '3465',\n",
       " '34th',\n",
       " '35',\n",
       " '35mm',\n",
       " '36',\n",
       " '360',\n",
       " '3654',\n",
       " '37',\n",
       " '37th',\n",
       " '38th',\n",
       " '39',\n",
       " '3d',\n",
       " '3p0',\n",
       " '3po',\n",
       " '3rd',\n",
       " '40',\n",
       " '400',\n",
       " '40mins',\n",
       " '40s',\n",
       " '41',\n",
       " '42',\n",
       " '426',\n",
       " '43',\n",
       " '44',\n",
       " '449',\n",
       " '45',\n",
       " '460',\n",
       " '47',\n",
       " '48',\n",
       " '48th',\n",
       " '49',\n",
       " '4am',\n",
       " '4th',\n",
       " '50',\n",
       " '500',\n",
       " '5000',\n",
       " '50000',\n",
       " '500k',\n",
       " '500th',\n",
       " '50s',\n",
       " '51',\n",
       " '52',\n",
       " '53',\n",
       " '54',\n",
       " '54th',\n",
       " '55',\n",
       " '56',\n",
       " '5671',\n",
       " '57',\n",
       " '571',\n",
       " '57th',\n",
       " '58',\n",
       " '59',\n",
       " '5th',\n",
       " '60',\n",
       " '600',\n",
       " '6000',\n",
       " '607',\n",
       " '60s',\n",
       " '61',\n",
       " '62',\n",
       " '63',\n",
       " '64',\n",
       " '640',\n",
       " '65',\n",
       " '65th',\n",
       " '66',\n",
       " '666',\n",
       " '67',\n",
       " '68',\n",
       " '69',\n",
       " '6th',\n",
       " '70',\n",
       " '700',\n",
       " '7000',\n",
       " '701',\n",
       " '70m',\n",
       " '70mm',\n",
       " '70s',\n",
       " '710',\n",
       " '712',\n",
       " '73',\n",
       " '747s',\n",
       " '75',\n",
       " '750',\n",
       " '76',\n",
       " '77',\n",
       " '777',\n",
       " '779',\n",
       " '78',\n",
       " '79',\n",
       " '7th',\n",
       " '80',\n",
       " '800',\n",
       " '802',\n",
       " '80s',\n",
       " '81',\n",
       " '82',\n",
       " '83',\n",
       " '84',\n",
       " '85',\n",
       " '85but',\n",
       " '86',\n",
       " '87',\n",
       " '88',\n",
       " '89',\n",
       " '8mm',\n",
       " '8th',\n",
       " '90',\n",
       " '900',\n",
       " '90210',\n",
       " '90s',\n",
       " '91',\n",
       " '911',\n",
       " '92',\n",
       " '92ll',\n",
       " '92re',\n",
       " '92s',\n",
       " '92t',\n",
       " '92ve',\n",
       " '93',\n",
       " '939',\n",
       " '94',\n",
       " '95',\n",
       " '95s',\n",
       " '96',\n",
       " '97',\n",
       " '98',\n",
       " '99',\n",
       " '999',\n",
       " '9mm',\n",
       " '_21_jump_street_',\n",
       " '___',\n",
       " '____',\n",
       " '_____',\n",
       " '______',\n",
       " '____________________________________________',\n",
       " '__________________________________________________________',\n",
       " '_a',\n",
       " '_a_night_at_the_roxbury_',\n",
       " '_air_force_one_',\n",
       " '_am_',\n",
       " '_amadeus_',\n",
       " '_america',\n",
       " '_american_beauty_',\n",
       " '_american_psycho_',\n",
       " '_and_',\n",
       " '_andre_',\n",
       " '_angel',\n",
       " '_animal',\n",
       " '_anything_',\n",
       " '_armageddon_',\n",
       " '_arrrgh_',\n",
       " '_babe_',\n",
       " '_basquiat_',\n",
       " '_before_',\n",
       " '_blade',\n",
       " '_boogie',\n",
       " '_boom_',\n",
       " '_breakfast_',\n",
       " '_breakfast_of_champions_',\n",
       " '_but',\n",
       " '_can',\n",
       " '_can_',\n",
       " '_casablanca_',\n",
       " '_cliffhanger',\n",
       " '_cliffhanger_',\n",
       " '_clueless_',\n",
       " '_come_',\n",
       " '_daylight_',\n",
       " '_dead_man_',\n",
       " '_dead_man_on_campus_',\n",
       " '_death',\n",
       " '_dirty_work_',\n",
       " '_do_',\n",
       " '_does_',\n",
       " '_don',\n",
       " '_dragon',\n",
       " '_dragonheart_',\n",
       " '_election',\n",
       " '_election_',\n",
       " '_entertainment_weekly_',\n",
       " '_eve',\n",
       " '_everybody_',\n",
       " '_exactly_',\n",
       " '_experience_',\n",
       " '_fantastic_',\n",
       " '_fear_and_loathing_in_las_vegas_',\n",
       " '_ferris',\n",
       " '_fifty_',\n",
       " '_film',\n",
       " '_flirting',\n",
       " '_full_house_',\n",
       " '_gattaca_',\n",
       " '_genius_',\n",
       " '_ghost',\n",
       " '_great_',\n",
       " '_happen_',\n",
       " '_have_',\n",
       " '_here_',\n",
       " '_highly_',\n",
       " '_home',\n",
       " '_hope',\n",
       " '_huge_',\n",
       " '_hustler_',\n",
       " '_in',\n",
       " '_into_',\n",
       " '_is_',\n",
       " '_itcom_',\n",
       " '_jerry_maguire_',\n",
       " '_kingpin_',\n",
       " '_la',\n",
       " '_last_',\n",
       " '_leave',\n",
       " '_life',\n",
       " '_long_',\n",
       " '_looks_',\n",
       " '_lot_',\n",
       " '_mafia_',\n",
       " '_matrix_',\n",
       " '_moby',\n",
       " '_more_',\n",
       " '_murder_',\n",
       " '_must_',\n",
       " '_no',\n",
       " '_not_',\n",
       " '_onegin_',\n",
       " '_original_',\n",
       " '_patlabor',\n",
       " '_patlabor_',\n",
       " '_pecker_',\n",
       " '_people_',\n",
       " '_pick_chucky_up_',\n",
       " '_polish_wedding_',\n",
       " '_pollock_',\n",
       " '_porky',\n",
       " '_practical',\n",
       " '_quite_',\n",
       " '_real_',\n",
       " '_reality',\n",
       " '_really_',\n",
       " '_red',\n",
       " '_remains',\n",
       " '_roxbury_',\n",
       " '_rushmore_',\n",
       " '_saturday_night_live_',\n",
       " '_saved_by_the_bell_',\n",
       " '_scarface_',\n",
       " '_scream',\n",
       " '_scream_',\n",
       " '_seven_nights_',\n",
       " '_shaft',\n",
       " '_shaft_',\n",
       " '_shaft_in_africa_',\n",
       " '_shine_',\n",
       " '_should_',\n",
       " '_six_days',\n",
       " '_snl_',\n",
       " '_so',\n",
       " '_some_',\n",
       " '_somewhere_',\n",
       " '_still_',\n",
       " '_survives_',\n",
       " '_that_',\n",
       " '_the',\n",
       " '_the_fugitive_',\n",
       " '_their_',\n",
       " '_there_',\n",
       " '_this_',\n",
       " '_titanic_',\n",
       " '_to',\n",
       " '_today_',\n",
       " '_too_',\n",
       " '_two_',\n",
       " '_unbreakable_',\n",
       " '_urban',\n",
       " '_very_',\n",
       " '_very_small_',\n",
       " '_whole_',\n",
       " '_will_',\n",
       " '_would_',\n",
       " 'aa',\n",
       " 'aaa',\n",
       " 'aaaaaaaaah',\n",
       " 'aaaaaaaahhhh',\n",
       " 'aaaaaah',\n",
       " 'aahs',\n",
       " 'aaliyah',\n",
       " 'aalyah',\n",
       " 'aardman',\n",
       " 'aaron',\n",
       " 'aatish',\n",
       " 'aback',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abandoning',\n",
       " 'abandonment',\n",
       " 'abandons',\n",
       " 'abba',\n",
       " 'abbe',\n",
       " 'abberation',\n",
       " 'abbots',\n",
       " 'abbott',\n",
       " 'abbotts',\n",
       " 'abby',\n",
       " 'abc',\n",
       " 'abdomen',\n",
       " 'abducted',\n",
       " 'abductees',\n",
       " 'abduction',\n",
       " 'abdul',\n",
       " 'abe',\n",
       " 'abel',\n",
       " 'aberration',\n",
       " 'abetted',\n",
       " 'abetting',\n",
       " 'abhorrence',\n",
       " 'abhorrent',\n",
       " 'abider',\n",
       " 'abides',\n",
       " 'abiding',\n",
       " 'abigail',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'abject',\n",
       " 'ablaze',\n",
       " 'able',\n",
       " 'ably',\n",
       " 'abo',\n",
       " 'aboard',\n",
       " 'abode',\n",
       " 'abolish',\n",
       " 'abolitionist',\n",
       " 'abolitionists',\n",
       " 'abomination',\n",
       " 'aborginal',\n",
       " 'aboriginal',\n",
       " 'aboriginals',\n",
       " 'aborted',\n",
       " 'abortion',\n",
       " 'abortionist',\n",
       " 'abortions',\n",
       " 'abortive',\n",
       " 'abound',\n",
       " 'abounded',\n",
       " 'abounding',\n",
       " 'abounds',\n",
       " 'about',\n",
       " 'abouts',\n",
       " 'above',\n",
       " 'abraded',\n",
       " 'abraham',\n",
       " 'abrahams',\n",
       " 'abrams',\n",
       " 'abrasive',\n",
       " 'abroad',\n",
       " 'abrupt',\n",
       " 'abruptly',\n",
       " 'abs',\n",
       " 'absconded',\n",
       " 'absence',\n",
       " 'absences',\n",
       " 'absense',\n",
       " 'absent',\n",
       " 'absentee',\n",
       " 'absinthe',\n",
       " 'absolut',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absolutes',\n",
       " 'absolution',\n",
       " 'absolutist',\n",
       " 'absorb',\n",
       " 'absorbant',\n",
       " 'absorbed',\n",
       " 'absorbing',\n",
       " 'absorbs',\n",
       " 'absorption',\n",
       " 'abstinence',\n",
       " 'abstract',\n",
       " 'absurd',\n",
       " 'absurdism',\n",
       " 'absurdist',\n",
       " 'absurdities',\n",
       " 'absurdity',\n",
       " 'absurdly',\n",
       " 'abu',\n",
       " 'abundance',\n",
       " 'abundant',\n",
       " 'abundantly',\n",
       " 'abundence',\n",
       " 'abuse',\n",
       " 'abused',\n",
       " 'abuser',\n",
       " 'abusers',\n",
       " 'abuses',\n",
       " 'abusing',\n",
       " 'abusive',\n",
       " 'abuzz',\n",
       " 'abysmal',\n",
       " 'abysmally',\n",
       " 'abyss',\n",
       " 'abyssinian',\n",
       " 'ac3',\n",
       " 'academia',\n",
       " 'academic',\n",
       " 'academics',\n",
       " 'academy',\n",
       " 'accelerate',\n",
       " 'accelerates',\n",
       " 'acceleration',\n",
       " 'accelerator',\n",
       " 'accent',\n",
       " 'accented',\n",
       " 'accents',\n",
       " 'accentuate',\n",
       " 'accentuates',\n",
       " 'accentuating',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'acceptance',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'accepts',\n",
       " 'access',\n",
       " 'accessibility',\n",
       " 'accessible',\n",
       " 'accessory',\n",
       " 'accident',\n",
       " 'accidental',\n",
       " 'accidentally',\n",
       " 'accidently',\n",
       " 'accidents',\n",
       " 'acclaim',\n",
       " 'acclaimed',\n",
       " 'acclimatize',\n",
       " 'accolade',\n",
       " 'accolades',\n",
       " 'accommodate',\n",
       " 'accommodating',\n",
       " 'accommodations',\n",
       " 'accomodates',\n",
       " 'accompanied',\n",
       " 'accompanies',\n",
       " 'accompaniment',\n",
       " 'accompany',\n",
       " 'accompanying',\n",
       " 'accomplice',\n",
       " 'accomplices',\n",
       " 'accomplish',\n",
       " 'accomplished',\n",
       " 'accomplishes',\n",
       " 'accomplishment',\n",
       " 'accomplishments',\n",
       " 'accord',\n",
       " 'accordance',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'accosted',\n",
       " 'accosts',\n",
       " 'account',\n",
       " 'accountability',\n",
       " 'accountable',\n",
       " 'accountant',\n",
       " 'accounted',\n",
       " 'accounts',\n",
       " 'accumulate',\n",
       " 'accumulated',\n",
       " 'accumulation',\n",
       " 'accuracy',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'accursed',\n",
       " 'accusation',\n",
       " 'accusations',\n",
       " 'accuse',\n",
       " 'accused',\n",
       " 'accuser',\n",
       " 'accuses',\n",
       " 'accusing',\n",
       " 'accustomed',\n",
       " 'ace',\n",
       " 'acerbic',\n",
       " 'acerbity',\n",
       " 'aces',\n",
       " 'ache',\n",
       " 'acheivement',\n",
       " 'acheives',\n",
       " 'aches',\n",
       " 'achievable',\n",
       " 'achieve',\n",
       " 'achieved',\n",
       " 'achieveing',\n",
       " 'achievement',\n",
       " 'achievements',\n",
       " 'achiever',\n",
       " 'achieves',\n",
       " 'achieving',\n",
       " 'achilles',\n",
       " 'achin',\n",
       " 'achingly',\n",
       " 'achoo',\n",
       " 'acid',\n",
       " 'acidic',\n",
       " 'aciton',\n",
       " 'ack',\n",
       " 'ackland',\n",
       " 'acknowledge',\n",
       " 'acknowledged',\n",
       " 'acknowledges',\n",
       " 'acknowledging',\n",
       " 'acknowledgment',\n",
       " 'acme',\n",
       " 'acne',\n",
       " 'acore',\n",
       " 'acquaintance',\n",
       " 'acquaintances',\n",
       " 'acquainted',\n",
       " 'acquaints',\n",
       " 'acquiescence',\n",
       " 'acquire',\n",
       " 'acquired',\n",
       " 'acquires',\n",
       " 'acquisition',\n",
       " 'acquit',\n",
       " 'acquits',\n",
       " 'acquittal',\n",
       " 'acquitted',\n",
       " 'acre',\n",
       " 'acres',\n",
       " 'acrimonious',\n",
       " 'acrimony',\n",
       " 'acrobat',\n",
       " 'acrobatic',\n",
       " 'acrobatics',\n",
       " 'acronym',\n",
       " 'across',\n",
       " 'act',\n",
       " 'acted',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'actioner',\n",
       " 'actioners',\n",
       " 'actionfest',\n",
       " 'actionless',\n",
       " 'actions',\n",
       " 'activated',\n",
       " 'active',\n",
       " 'actively',\n",
       " 'activist',\n",
       " 'activists',\n",
       " 'activites',\n",
       " 'activities',\n",
       " 'activity',\n",
       " 'actor',\n",
       " 'actors',\n",
       " 'actosta',\n",
       " 'actress',\n",
       " 'actresses',\n",
       " 'acts',\n",
       " 'actual',\n",
       " 'actualisation',\n",
       " 'actuality',\n",
       " 'actualization',\n",
       " 'actualizing',\n",
       " 'actually',\n",
       " 'acumen',\n",
       " 'acupuncture',\n",
       " 'acute',\n",
       " 'acutely',\n",
       " 'ad',\n",
       " 'ad2am',\n",
       " 'adafarasin',\n",
       " 'adage',\n",
       " 'adair',\n",
       " 'adam',\n",
       " 'adamantly',\n",
       " 'adams',\n",
       " 'adapt',\n",
       " 'adaptable',\n",
       " 'adaptation',\n",
       " 'adaptations',\n",
       " 'adapted',\n",
       " 'adapters',\n",
       " 'adapting',\n",
       " 'adaption',\n",
       " 'adaptions',\n",
       " 'aday',\n",
       " 'add',\n",
       " 'addams',\n",
       " 'added',\n",
       " 'addict',\n",
       " 'addicted',\n",
       " 'addiction',\n",
       " 'addictions',\n",
       " 'addictive',\n",
       " 'addicts',\n",
       " 'adding',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'additionally',\n",
       " 'additions',\n",
       " 'additive',\n",
       " 'addled',\n",
       " 'address',\n",
       " 'addressed',\n",
       " 'addresses',\n",
       " 'addressing',\n",
       " 'adds',\n",
       " 'addy',\n",
       " 'ade',\n",
       " 'adefarasin',\n",
       " 'adelaide',\n",
       " 'adele',\n",
       " 'adept',\n",
       " 'adeptly',\n",
       " 'adequate',\n",
       " 'adequately',\n",
       " 'adhere',\n",
       " 'adherence',\n",
       " 'adherents',\n",
       " 'adheres',\n",
       " 'adhesive',\n",
       " 'adian',\n",
       " 'aditi',\n",
       " 'aditya',\n",
       " 'adjacent',\n",
       " 'adjani',\n",
       " 'adjective',\n",
       " 'adjectives',\n",
       " 'adjoining',\n",
       " 'adjust',\n",
       " 'adjuster',\n",
       " 'adjusting',\n",
       " 'adjustment',\n",
       " 'adjustments',\n",
       " 'adjusts',\n",
       " 'adlai',\n",
       " 'adlib',\n",
       " 'administered',\n",
       " 'administering',\n",
       " 'administration',\n",
       " 'administrator',\n",
       " 'admirable',\n",
       " 'admirably',\n",
       " 'admiral',\n",
       " 'admirals',\n",
       " 'admiration',\n",
       " 'admire',\n",
       " 'admired',\n",
       " 'admirer',\n",
       " 'admirers',\n",
       " 'admires',\n",
       " 'admiring',\n",
       " 'admission',\n",
       " 'admit',\n",
       " 'admiting',\n",
       " 'admits',\n",
       " 'admittance',\n",
       " 'admitted',\n",
       " 'admittedly',\n",
       " 'admitting',\n",
       " 'admittingly',\n",
       " 'admonition',\n",
       " 'ado',\n",
       " 'adolescence',\n",
       " 'adolescent',\n",
       " 'adolescents',\n",
       " 'adolph',\n",
       " 'adopt',\n",
       " 'adopted',\n",
       " 'adopter',\n",
       " 'adopting',\n",
       " 'adoption',\n",
       " 'adoptive',\n",
       " 'adopts',\n",
       " 'adorable',\n",
       " 'adorableness',\n",
       " 'adorably',\n",
       " 'adoration',\n",
       " 'adore',\n",
       " 'adored',\n",
       " 'adores',\n",
       " 'adoring',\n",
       " 'adorn',\n",
       " 'adorned',\n",
       " 'adrenalin',\n",
       " 'adrenaline',\n",
       " 'adrian',\n",
       " 'adriana',\n",
       " 'adrianne',\n",
       " 'adrien',\n",
       " 'adrienne',\n",
       " 'adrift',\n",
       " 'adroit',\n",
       " 'adroitly',\n",
       " 'ads',\n",
       " 'adulation',\n",
       " 'adult',\n",
       " 'adulterer',\n",
       " 'adulterous',\n",
       " 'adultery',\n",
       " 'adulthood',\n",
       " 'adults',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advancement',\n",
       " 'advancements',\n",
       " 'advances',\n",
       " 'advancing',\n",
       " 'advantage',\n",
       " 'advantaged',\n",
       " 'advantages',\n",
       " 'advent',\n",
       " 'adventure',\n",
       " 'adventurer',\n",
       " 'adventures',\n",
       " 'adventurous',\n",
       " 'adversarial',\n",
       " 'adversary',\n",
       " 'adverse',\n",
       " 'adversely',\n",
       " 'adversity',\n",
       " 'advertise',\n",
       " 'advertised',\n",
       " 'advertisement',\n",
       " 'advertisements',\n",
       " 'advertiser',\n",
       " 'advertising',\n",
       " 'advice',\n",
       " 'advil',\n",
       " 'advisable',\n",
       " 'advise',\n",
       " 'advised',\n",
       " 'adviser',\n",
       " 'advisers',\n",
       " 'advises',\n",
       " 'advising',\n",
       " 'advisor',\n",
       " 'advisors',\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the fitted vocabulary\n",
    "vocabulary = vect.get_feature_names()\n",
    "print('number of words in the vocabulary: {} '.format(len(vocabulary)))\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1500x35152 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 495843 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform training data into a \"document-term matrix'\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "X_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 175)\t5\n",
      "  (0, 206)\t3\n",
      "  (0, 586)\t1\n",
      "  (0, 607)\t3\n",
      "  (0, 1062)\t2\n",
      "  (0, 1280)\t3\n",
      "  (0, 1330)\t1\n",
      "  (0, 1332)\t1\n",
      "  (0, 1348)\t3\n",
      "  (0, 1470)\t1\n",
      "  (0, 1509)\t1\n",
      "  (0, 1548)\t13\n",
      "  (0, 1659)\t2\n",
      "  (0, 1670)\t1\n",
      "  (0, 1760)\t1\n",
      "  (0, 1856)\t1\n",
      "  (0, 1919)\t6\n",
      "  (0, 1975)\t1\n",
      "  (0, 1995)\t2\n",
      "  (0, 2022)\t1\n",
      "  (0, 2024)\t1\n",
      "  (0, 2036)\t1\n",
      "  (0, 2076)\t2\n",
      "  (0, 2233)\t3\n",
      "  (0, 2461)\t1\n",
      "  :\t:\n",
      "  (1499, 34370)\t1\n",
      "  (1499, 34385)\t4\n",
      "  (1499, 34395)\t1\n",
      "  (1499, 34417)\t1\n",
      "  (1499, 34421)\t1\n",
      "  (1499, 34423)\t1\n",
      "  (1499, 34494)\t1\n",
      "  (1499, 34580)\t1\n",
      "  (1499, 34592)\t9\n",
      "  (1499, 34605)\t1\n",
      "  (1499, 34649)\t1\n",
      "  (1499, 34704)\t1\n",
      "  (1499, 34708)\t1\n",
      "  (1499, 34712)\t1\n",
      "  (1499, 34724)\t2\n",
      "  (1499, 34743)\t1\n",
      "  (1499, 34753)\t1\n",
      "  (1499, 34757)\t1\n",
      "  (1499, 34798)\t1\n",
      "  (1499, 34821)\t2\n",
      "  (1499, 34861)\t1\n",
      "  (1499, 34927)\t2\n",
      "  (1499, 34995)\t18\n",
      "  (1499, 35003)\t7\n",
      "  (1499, 35006)\t1\n"
     ]
    }
   ],
   "source": [
    "# examine the content of the sparse matrix\n",
    "print(X_train_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0009f</th>\n",
       "      <th>007</th>\n",
       "      <th>00s</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>05425</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>zucker</th>\n",
       "      <th>zuehlke</th>\n",
       "      <th>zuko</th>\n",
       "      <th>zulu</th>\n",
       "      <th>zurg</th>\n",
       "      <th>zus</th>\n",
       "      <th>zweibel</th>\n",
       "      <th>zwick</th>\n",
       "      <th>zwigoff</th>\n",
       "      <th>zycie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1470</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1471</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1473</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1475</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1477</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1478</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1480</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1483</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1491</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1493</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1494</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 35152 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      00  000  0009f  007  00s  03  04  05  05425  10  ...    zucker  zuehlke  \\\n",
       "0      0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "1      0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "2      0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "3      0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "4      0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "5      0    0      0    0    0   0   0   0      0   8  ...         0        0   \n",
       "6      0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "7      0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "8      0    0      0    0    0   0   0   0      0   1  ...         0        0   \n",
       "9      0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "10     0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "11     0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "12     0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "13     0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "14     0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "15     0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "16     0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "17     0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "18     0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "19     0    2      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "20     0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "21     0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "22     0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "23     0    1      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "24     0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "25     0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "26     0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "27     0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "28     0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "29     0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "...   ..  ...    ...  ...  ...  ..  ..  ..    ...  ..  ...       ...      ...   \n",
       "1470   0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "1471   0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "1472   0    1      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "1473   0    0      0    0    0   0   0   0      0   5  ...         0        0   \n",
       "1474   0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "1475   0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "1476   0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "1477   0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "1478   0    0      0    0    0   0   0   0      0   1  ...         0        0   \n",
       "1479   0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "1480   0    0      0    0    0   0   0   0      0   1  ...         0        0   \n",
       "1481   0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "1482   0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "1483   0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "1484   0    1      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "1485   0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "1486   0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "1487   0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "1488   0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "1489   0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "1490   0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "1491   0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "1492   0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "1493   0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "1494   0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "1495   0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "1496   0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "1497   0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "1498   0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "1499   0    0      0    0    0   0   0   0      0   0  ...         0        0   \n",
       "\n",
       "      zuko  zulu  zurg  zus  zweibel  zwick  zwigoff  zycie  \n",
       "0        0     0     0    0        0      0        0      0  \n",
       "1        0     0     0    0        0      0        0      0  \n",
       "2        0     0     0    0        0      0        0      0  \n",
       "3        0     0     0    0        0      0        0      0  \n",
       "4        0     0     0    0        0      0        0      0  \n",
       "5        0     0     0    0        0      0        0      0  \n",
       "6        0     0     0    0        0      0        0      0  \n",
       "7        0     0     0    0        0      0        0      0  \n",
       "8        0     0     0    0        0      0        0      0  \n",
       "9        0     0     0    0        0      0        0      0  \n",
       "10       0     0     0    0        0      0        0      0  \n",
       "11       0     0     0    0        0      0        0      0  \n",
       "12       0     0     0    0        0      0        0      0  \n",
       "13       0     0     0    0        0      0        0      0  \n",
       "14       0     0     0    0        0      0        0      0  \n",
       "15       0     0     0    0        0      0        0      0  \n",
       "16       0     0     0    0        0      0        0      0  \n",
       "17       0     0     0    0        0      0        0      0  \n",
       "18       0     0     0    0        0      0        0      0  \n",
       "19       0     0     0    0        0      0        0      0  \n",
       "20       0     0     0    0        0      0        0      0  \n",
       "21       0     0     0    0        0      0        0      0  \n",
       "22       0     0     0    0        0      0        0      0  \n",
       "23       0     0     0    0        0      0        0      0  \n",
       "24       0     0     0    0        0      0        0      0  \n",
       "25       0     0     0    0        0      0        0      0  \n",
       "26       0     0     0    0        0      0        0      0  \n",
       "27       0     0     0    0        0      0        0      0  \n",
       "28       0     0     0    0        0      0        0      0  \n",
       "29       0     0     0    0        0      0        0      0  \n",
       "...    ...   ...   ...  ...      ...    ...      ...    ...  \n",
       "1470     0     0     0    0        0      0        0      0  \n",
       "1471     0     0     0    0        0      0        0      0  \n",
       "1472     0     0     0    0        0      0        0      0  \n",
       "1473     0     0     0    0        0      0        0      0  \n",
       "1474     0     0     0    0        0      0        0      0  \n",
       "1475     0     0     0    0        0      0        0      0  \n",
       "1476     0     0     0    0        0      0        0      0  \n",
       "1477     0     0     0    0        0      0        0      0  \n",
       "1478     0     0     0    0        0      0        0      0  \n",
       "1479     0     0     0    0        0      0        0      0  \n",
       "1480     0     0     0    0        0      0        0      0  \n",
       "1481     0     0     0    0        0      0        0      0  \n",
       "1482     0     0     0    0        0      0        0      0  \n",
       "1483     0     0     0    0        0      0        0      0  \n",
       "1484     0     0     0    0        0      0        0      0  \n",
       "1485     0     0     0    0        0      0        0      0  \n",
       "1486     0     0     0    0        0      0        0      0  \n",
       "1487     0     0     0    0        0      0        0      0  \n",
       "1488     0     0     0    0        0      0        0      0  \n",
       "1489     0     0     0    0        0      0        0      0  \n",
       "1490     0     0     0    0        0      0        0      0  \n",
       "1491     0     0     0    0        0      0        0      0  \n",
       "1492     0     0     0    0        0      0        0      0  \n",
       "1493     0     0     0    0        0      0        0      0  \n",
       "1494     0     0     0    0        0      0        0      0  \n",
       "1495     0     0     0    0        0      0        0      0  \n",
       "1496     0     0     0    0        0      0        0      0  \n",
       "1497     0     0     0    0        0      0        0      0  \n",
       "1498     0     0     0    0        0      0        0      0  \n",
       "1499     0     0     0    0        0      0        0      0  \n",
       "\n",
       "[1500 rows x 35152 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the vocabulary and document-term matrix together (X_train_dtm.toarray() converts sparse matrix to a dense matrix)\n",
    "pd.DataFrame(X_train_dtm.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<500x35152 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 165969 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "X_test_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class prediction with Multinomial Naive Bayes\n",
    "The first classifier we test here is a classical one for textual data represented as bag of words, and the combination of these two was a reference until the explosion of deep learning. So it's a good start to have a feeling of the performance we can get on our dataset. The use is really simple with scikit-learn, since it stands in 3 lines for the training and 1 line for a prediction on test data. More, this method has the advantage to have really low computation time, and not to require lots of training data like deep learning methods. It has 3 parameters (from scikit-learn documentation):\n",
    " - alpha: additive (Laplace/Lidstone) smoothing parameter (default=1.0, 0 for no smoothing)\n",
    " - fit_prior: whether to learn class prior probabilities or not. If false, a uniform prior will be used (default=True)\n",
    " - class_prior: enables to give one's own prior probabilities to the classes. If specified the priors are not adjusted according to the data (default=None)\n",
    "\n",
    "Here we already know that the 2 classes are equaly distributed, so the fit_prior parameter has no impact: we will get the same results by computing prior probabilities or by imposing a uniform distribution. And we don't try other priors either as it would give worse results. However alpha has a real impact that we evaluate in the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scikit-learn library\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a Multinomial Naive Bayes model\n",
    "alpha = 2 #(default=1.0, 0 for no smoothing)\n",
    "MNB = MultinomialNB(alpha, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 80.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=2, class_prior=None, fit_prior=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model using X_train_dtm (timing it with an IPython \"magic command\")\n",
    "%time MNB.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class predictions for X_test_dtm\n",
    "y_pred_class = MNB.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation\n",
    "We evaluate each classifier with common metrics: accuracy and area under ROC curve, and also compute the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 0.808\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy of class predictions\n",
    "accuracy_MNB = metrics.accuracy_score(y_test, y_pred_class)\n",
    "print('accuracy is: {}'.format(accuracy_MNB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "area under ROC curve is: 0.884\n"
     ]
    }
   ],
   "source": [
    "# calculate predicted probabilities for X_test_dtm and AUC\n",
    "y_pred_prob = MNB.predict_proba(X_test_dtm)[:, 1]\n",
    "AUC_MNB = metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "print('area under ROC curve is: {}'.format(round(AUC_MNB,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix is:\n",
      "[[195  50]\n",
      " [ 46 209]]\n"
     ]
    }
   ],
   "source": [
    "# print the confusion matrix\n",
    "conf_mat_MNB = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "print('confusion matrix is:')\n",
    "print(conf_mat_MNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 50 false positive samples:\n",
      "211    the rich legacy of cinema has left us with cer...\n",
      "683    often similar to a little boy lost in a park t...\n",
      "822    it's difficult to expect much from a director ...\n",
      "927    star wars : ? episode i -- the phantom menace ...\n",
      "762    weighed down by tired plot lines and spielberg...\n",
      "481    as any reasonable human being would , i must a...\n",
      "87     lucas was wise to start his star wars trilogy ...\n",
      "315    since director steven zaillian previously wrot...\n",
      "238    there is a scene early in jakob the liar that ...\n",
      "541    whether or not i would be considered a trekker...\n",
      "244    i read the new yorker magazine and i enjoy som...\n",
      "110    nearly every film tim burton has directed has ...\n",
      "552    kate ( jennifer aniston ) is having some probl...\n",
      "168    so , it's thirty years later , and oscar and f...\n",
      "263    it's not a bad thing to update old stories . \\...\n",
      "603    by phil curtolo \" madonna - antonio banderas -...\n",
      "216    libby parsons ( ashley judd ) has the perfect ...\n",
      "786    for a film touted as exploring relationships a...\n",
      "268    i have always been a fan of director neil jord...\n",
      "635    i didn't come into city of angels expecting gr...\n",
      "529    here's a rarity : a children's film that attem...\n",
      "42     out of sight director steven sorderbergh baffl...\n",
      "377    robin williams has the rarest of gifts : the a...\n",
      "806    my first exposure to the nightmare on elm stre...\n",
      "905    it's a good thing most animated sci-fi movies ...\n",
      "640    i'll be the first to admit it . \\nwhen you men...\n",
      "736    this well-conceived but ultra sugary coming-of...\n",
      "647     \" with all that education , you should know w...\n",
      "199    this independent film written and directed by ...\n",
      "9      call it a road trip for the walking wounded . ...\n",
      "179    starring ben stiller , elizabeth hurley , mari...\n",
      "900    in life , eddie murphy and martin lawrence pla...\n",
      "380    in my review of \" the spy who shagged me , \" i...\n",
      "476    miramax \" disinvited \" on-line media from pres...\n",
      "302    carry on at your convenience is all about the ...\n",
      "652    the real blonde ( r ) a woman's face , an arm ...\n",
      "382    for a movie with such deep religious and spiri...\n",
      "976    stars : armand assante ( mike hammer ) , barba...\n",
      "82     coinciding with the emerging popularity of mov...\n",
      "853    delicatessen ( directors : marc caro/jean-pier...\n",
      "668     \" the red violin \" is a cold , sterile featur...\n",
      "676    have you ever been in an automobile accident w...\n",
      "632    the swooping shots across darkened rooftops su...\n",
      "672    david schwimmer ( from the television series \"...\n",
      "973    'bicentennial man' is a family film without an...\n",
      "992    the king and i , a warner brothers animated , ...\n",
      "804    note : some may consider portions of the follo...\n",
      "354     \" the 44 caliber killer has struck again . \" ...\n",
      "91     various films seen at the seattle film festiva...\n",
      "530    there's no reason to doubt that donnie brasco ...\n",
      "Name: content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print reviews for the false positives\n",
    "FP_MNB = X_test[(y_pred_class==1) & (y_test==-1)]\n",
    "print('there are {} false positive samples:'.format(len(FP_MNB)))\n",
    "print(FP_MNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 46 false negative samples:\n",
      "859    mike myers , you certainly did throw us a ? fr...\n",
      "882    the word 'rest' in the title should be stresse...\n",
      "283    ok , let's get one thing straight right away :...\n",
      "694    let me first say that the conditions that i wa...\n",
      "656    jacques tati's 1953 classic \" les vacances de ...\n",
      "127    i have to say it . \\ntim burton's retelling of...\n",
      "237    jerry springer has got nothing on \" wild thing...\n",
      "373    okay , let me first say , this is a beavis and...\n",
      "995    a thriller set in modern day seattle , that ma...\n",
      "987    i think the first thing this reviewer should m...\n",
      "701    let me start off by saying that leading up to ...\n",
      "556    harmless , silly and fun comedy about dim-witt...\n",
      "762    film adaptation of hunter s . thompson's infam...\n",
      "349    bob the happy bastard's quickie review : \\nthe...\n",
      "481    the trailers and the beginning of the move sum...\n",
      "511    are we victims of fate in life or can we creat...\n",
      "390    when i left the theater after seeing david lyn...\n",
      "988    trees lounge is the directoral debut from one ...\n",
      "195    it's a fact that a good thriller or action mov...\n",
      "76     well , i know that stallone is 50 years old no...\n",
      "293    the muppet movie is the first , and the best m...\n",
      "120    disaster films have a tendency to be very form...\n",
      "862    an indian runner was more than a courier . \\nh...\n",
      "953    director dominic sena ( who made the highly un...\n",
      "224    because the press screening of \" planet of the...\n",
      "508    capsule : side-splitting comedy that follows i...\n",
      "397    well i'll be damned , what a most excellent su...\n",
      "83     a big surprise to me . \\nthe good trailer had ...\n",
      "105    based on the boris karloff's classic by the sa...\n",
      "433    based on the relatively unknown ( in compariso...\n",
      "404    my fellow americans is a movie that at first g...\n",
      "108    is evil dead ii a bad movie ? \\nit's full of t...\n",
      "703    the characters in \" palmetto \" collectively sw...\n",
      "260    richard linklater's \" slacker , \" made in 1991...\n",
      "50     warning : contains what the matrix is . \\nrate...\n",
      "401    the happy bastard's 30-second review : \\nameri...\n",
      "1      films adapted from comic books have had plenty...\n",
      "817    in my review of there's something about mary ,...\n",
      "791    tempe mills cinema , az--this movie had us in ...\n",
      "306    of all the films i've come to see this year ( ...\n",
      "992    i don't box with kid gloves . \\ni don't play n...\n",
      "81     just how inseparable is the team of sgt . \\nma...\n",
      "608    us critic-type people are always shaking our h...\n",
      "465    swashbuckling adventure that can be enjoyed by...\n",
      "447     \" desperate measures \" was something i was ex...\n",
      "41      \" he's back , and it's about time . \" \\nwas t...\n",
      "Name: content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print reviews for the false negatives\n",
    "FN_MNB = X_test[(y_pred_class==-1) & (y_test==1)]\n",
    "print('there are {} false negative samples:'.format(len(FN_MNB)))\n",
    "print(FN_MNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning with cross-validation\n",
    "We evaluate the impact of the parameter alpha and try to find the best value with grid search cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameters\n",
    "alpha_range = [1.0e-10, 0.25, 0.5, 1, 1.5, 2, 3, 4, 6, 8] #0 not accepted in MultinomialNB and automatically mapped to 1.0e-10\n",
    "param_grid = dict(alpha=alpha_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'alpha': [1e-10, 0.25, 0.5, 1, 1.5, 2, 3, 4, 6, 8]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# operate grid search with default RBF kernel\n",
    "grid = GridSearchCV(MultinomialNB(), param_grid=param_grid, cv=3, return_train_score=True)\n",
    "grid.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best alpha is {'alpha': 2} with a score of 0.81\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.68733333, 0.78333333, 0.79066667, 0.796     , 0.80733333,\n",
       "       0.81266667, 0.806     , 0.786     , 0.75866667, 0.72333333])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print results\n",
    "print(\"The best alpha is %s with a score of %0.2f\" % (grid.best_params_, grid.best_score_))\n",
    "scores = grid.cv_results_['mean_test_score']\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot scores\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(alpha_range, scores)\n",
    "plt.xlabel('alpha')\n",
    "plt.title('Multinomial Naive Bayes: validation accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class prediction with logistic regression\n",
    "A second common classification method is logistic regression, which is also provided by scikit-learn. The main parameters are (from scikit-learn documentation):\n",
    " - solver: algorithm to use in the optimization problem, among ‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’ (default: ‘liblinear’, but will be changed to 'lbfgs' in scikit-learn v0.22)\n",
    " - max_iter: maximum number of iterations taken for the solvers ‘newton-cg’, ‘sag’ and ‘lbfgs’ to converge\n",
    " - penalty: norm used in the penalization (‘l1’ or ‘l2’, default: ‘l2’).\n",
    " - dual: use of dual or primal formulation (default: False). Dual formulation is only implemented for l2 penalty.\n",
    " - tol: tolerance for stopping criteria (default: 1e-4)\n",
    " - C: inverse of regularization strength (default: 1.0). Like in SVMs, smaller values specify stronger regularization.\n",
    " - fit_intercept: whether a constant (a.k.a. bias or intercept) should be added to the decision function (default: True)\n",
    " - \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and instantiate a logistic regression model \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 705 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model using X_train_dtm\n",
    "%time logreg.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class predictions for X_test_dtm\n",
    "y_pred_class = logreg.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 0.842\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "accuracy_LogReg = metrics.accuracy_score(y_test, y_pred_class)\n",
    "print('accuracy is: {}'.format(accuracy_LogReg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "area under ROC curve is: 0.912\n"
     ]
    }
   ],
   "source": [
    "# calculate predicted probabilities for X_test_dtm and area under ROC curve\n",
    "y_pred_prob = logreg.predict_proba(X_test_dtm)[:, 1]\n",
    "AUC_LogReg = metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "print('area under ROC curve is: {}'.format(round(AUC_LogReg,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix is:\n",
      "[[211  34]\n",
      " [ 45 210]]\n"
     ]
    }
   ],
   "source": [
    "# print the confusion matrix\n",
    "conf_mat_LogReg = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "print('confusion matrix is:')\n",
    "print(conf_mat_LogReg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with another solver\n",
    "Default solver will be changed to 'lbfgs' in scikit-learn v0.22. So it may be useful to try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a logistic regression model \n",
    "logreg = LogisticRegression(solver='lbfgs', max_iter=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.97 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=300, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model using X_train_dtm\n",
    "%time logreg.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class predictions for X_test_dtm\n",
    "y_pred_class = logreg.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 0.836\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "accuracy_LogReg = metrics.accuracy_score(y_test, y_pred_class)\n",
    "print('accuracy is: {}'.format(accuracy_LogReg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "area under ROC curve is: 0.912\n"
     ]
    }
   ],
   "source": [
    "# calculate predicted probabilities for X_test_dtm and area under ROC curve\n",
    "y_pred_prob = logreg.predict_proba(X_test_dtm)[:, 1]\n",
    "AUC_LogReg = metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "print('area under ROC curve is: {}'.format(round(AUC_LogReg,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix is:\n",
      "[[210  35]\n",
      " [ 47 208]]\n"
     ]
    }
   ],
   "source": [
    "# print the confusion matrix\n",
    "conf_mat_LogReg = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "print('confusion matrix is:')\n",
    "print(conf_mat_LogReg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning parameters with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_range = np.logspace(0, 10, 6)\n",
    "gamma_range = np.logspace(-9, -1, 5)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# operate grid search\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=3, return_train_score=True)\n",
    "grid.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "print(\"The best parameters are %s with a score of %0.2f\" % (grid.best_params_, grid.best_score_))\n",
    "scores = grid.cv_results_['mean_test_score'].reshape(len(C_range),len(gamma_range))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw heatmap\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot)\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('C')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)\n",
    "plt.yticks(np.arange(len(C_range)), C_range)\n",
    "plt.title('Validation accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class prediction with SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import, instantiate and train a SVM model without probability estimation\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='linear')\n",
    "%time clf.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class predictions for X_test_dtm\n",
    "y_pred_class = clf.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the confusion matrix\n",
    "metrics.confusion_matrix(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import, instantiate and train a SVM model with probability estimation\n",
    "clf = SVC(kernel='linear', probability=True)\n",
    "%time clf.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate predicted probabilities for X_test_dtm\n",
    "y_pred_prob = clf.predict_proba(X_test_dtm)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate AUC\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning parameters with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st step: large logarithmic grid search\n",
    "C_range = np.logspace(0, 10, 6)\n",
    "gamma_range = np.logspace(-9, -1, 5)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# operate grid search with default RBF kernel\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=3, return_train_score=True)\n",
    "grid.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "print(\"The best parameters are %s with a score of %0.2f\" % (grid.best_params_, grid.best_score_))\n",
    "scores = grid.cv_results_['mean_test_score'].reshape(len(C_range),len(gamma_range))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw heatmap\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot)\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('C')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)\n",
    "plt.yticks(np.arange(len(C_range)), C_range)\n",
    "plt.title('Validation accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd step: precise logarithmic grid search on selected range\n",
    "C_range = np.logspace(4, 8, 5)\n",
    "gamma_range = np.logspace(-8, -4, 5)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "grid = GridSearchCV(SVC(kernel='linear'), param_grid=param_grid)\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# operate grid search with default RBF kernel\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=3, return_train_score=True)\n",
    "grid.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "print(\"The best parameters are %s with a score of %0.2f\" % (grid.best_params_, grid.best_score_))\n",
    "scores = grid.cv_results_['mean_test_score'].reshape(len(C_range),len(gamma_range))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot)\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('C')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)\n",
    "plt.yticks(np.arange(len(C_range)), C_range)\n",
    "plt.title('Validation accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class prediction using bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.values.reshape(1, y_train.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_dtm.toarray().reshape(1, X_train_dtm.shape[0], X_train_dtm.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 1, 35604)\n",
      "[[[[1], [-1], [1], [1], [1], [-1], [-1], [1], [1], [-1], [-1], [1], [1], [-1], [-1], [1], [-1], [-1], [1], [1], [1], [1], [1], [-1], [-1], [1], [-1], [1], [1], [1], [-1], [-1], [1], [1], [1], [1], [1], [1], [-1], [-1], [-1], [1], [-1], [1], [-1], [1], [-1], [1], [-1], [-1], [-1], [-1], [1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [1], [-1], [-1], [-1], [-1], [-1], [-1], [1], [-1], [-1], [1], [-1], [-1], [-1], [1], [-1], [1], [-1], [-1], [-1], [-1], [1], [1], [1], [1], [-1], [-1], [1], [-1], [-1], [-1], [1], [1], [1], [-1], [-1], [-1], [1], [1], [-1], [-1], [1], [-1], [-1], [-1], [-1], [1], [-1], [-1], [1], [-1], [-1], [1], [1], [1], [-1], [-1], [-1], [-1], [1], [-1], [1], [1], [1], [1], [1], [1], [-1], [1], [1], [-1], [1], [-1], [-1], [1], [1], [1], [-1], [-1], [-1], [-1], [1], [-1], [-1], [-1], [-1], [1], [1], [1], [1], [-1], [-1], [1], [1], [-1], [1], [-1], [-1], [-1], [1], [-1], [1], [1], [-1], [-1], [-1], [-1], [1], [1], [1], [-1], [1], [-1], [1], [-1], [1], [-1], [1], [-1], [1], [1], [1], [1], [1], [1], [1], [-1], [-1], [-1], [1], [1], [-1], [-1], [1], [-1], [-1], [-1], [1], [-1], [1], [1], [1], [-1], [1], [1], [1], [1], [1], [1], [-1], [-1], [-1], [-1], [-1], [1], [-1], [1], [1], [1], [1], [1], [-1], [-1], [1], [-1], [-1], [1], [1], [-1], [1], [1], [1], [1], [-1], [1], [-1], [-1], [1], [-1], [1], [-1], [-1], [1], [-1], [1], [-1], [1], [-1], [-1], [-1], [1], [-1], [1], [1], [-1], [1], [-1], [1], [-1], [1], [-1], [1], [1], [-1], [1], [-1], [-1], [-1], [1], [1], [1], [-1], [1], [1], [-1], [-1], [1], [-1], [1], [1], [-1], [-1], [-1], [1], [1], [-1], [1], [1], [1], [1], [1], [1], [-1], [-1], [1], [-1], [1], [-1], [1], [-1], [-1], [-1], [1], [1], [-1], [1], [-1], [1], [-1], [1], [-1], [-1], [-1], [1], [1], [1], [-1], [1], [-1], [1], [-1], [1], [-1], [-1], [1], [1], [1], [1], [1], [1], [1], [1], [-1], [-1], [1], [-1], [-1], [1], [-1], [1], [-1], [1], [-1], [-1], [-1], [-1], [-1], [-1], [1], [1], [1], [1], [-1], [1], [1], [1], [-1], [-1], [1], [-1], [-1], [1], [-1], [-1], [-1], [-1], [1], [-1], [-1], [-1], [1], [1], [-1], [1], [1], [-1], [-1], [1], [1], [1], [1], [1], [1], [1], [1], [-1], [-1], [1], [1], [-1], [1], [-1], [1], [-1], [-1], [1], [1], [1], [-1], [-1], [1], [1], [1], [1], [-1], [-1], [1], [-1], [-1], [1], [1], [-1], [1], [-1], [1], [1], [1], [-1], [-1], [-1], [-1], [-1], [1], [1], [-1], [1], [-1], [1], [-1], [-1], [1], [1], [-1], [1], [1], [-1], [-1], [1], [-1], [1], [1], [-1], [1], [-1], [1], [-1], [1], [-1], [1], [-1], [1], [1], [1], [1], [1], [1], [1], [-1], [-1], [-1], [-1], [-1], [1], [1], [-1], [-1], [-1], [-1], [1], [-1], [-1], [1], [1], [-1], [-1], [-1], [-1], [-1], [1], [1], [-1], [1], [-1], [-1], [-1], [-1], [1], [1], [1], [1], [-1], [-1], [-1], [-1], [1], [1], [1], [-1], [1], [-1], [1], [1], [-1], [1], [-1], [1], [1], [-1], [-1], [1], [1], [-1], [-1], [1], [-1], [1], [-1], [-1], [-1], [1], [-1], [1], [1], [-1], [-1], [1], [1], [-1], [1], [-1], [1], [-1], [-1], [1], [-1], [1], [-1], [1], [-1], [-1], [-1], [1], [1], [-1], [1], [1], [-1], [-1], [-1], [1], [1], [-1], [1], [1], [-1], [-1], [1], [-1], [1], [1], [1], [1], [-1], [-1], [-1], [1], [1], [1], [1], [1], [-1], [-1], [-1], [1], [1], [1], [-1], [-1], [-1], [-1], [-1], [-1], [1], [1], [1], [1], [1], [1], [-1], [1], [-1], [-1], [-1], [-1], [1], [-1], [1], [-1], [1], [1], [-1], [-1], [-1], [1], [-1], [-1], [1], [-1], [1], [1], [1], [-1], [-1], [-1], [-1], [1], [-1], [-1], [-1], [1], [-1], [1], [-1], [-1], [1], [1], [1], [-1], [1], [-1], [1], [1], [-1], [-1], [-1], [1], [-1], [1], [1], [-1], [-1], [-1], [-1], [1], [1], [1], [1], [1], [1], [-1], [1], [1], [-1], [-1], [-1], [-1], [1], [-1], [1], [-1], [-1], [-1], [-1], [1], [1], [-1], [1], [1], [1], [1], [-1], [1], [-1], [1], [-1], [-1], [1], [1], [1], [1], [-1], [-1], [1], [-1], [-1], [-1], [-1], [-1], [-1], [1], [-1], [1], [-1], [-1], [1], [1], [1], [1], [1], [-1], [-1], [-1], [1], [1], [-1], [1], [1], [-1], [1], [1], [1], [1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [1], [-1], [-1], [-1], [-1], [1], [1], [1], [1], [-1], [1], [-1], [1], [1], [1], [1], [1], [-1], [1], [1], [-1], [-1], [-1], [-1], [1], [1], [1], [-1], [1], [1], [1], [-1], [1], [-1], [-1], [1], [1], [1], [1], [1], [-1], [1], [1], [1], [-1], [1], [-1], [-1], [1], [-1], [1], [-1], [-1], [1], [1], [1], [1], [1], [-1], [-1], [1], [1], [-1], [-1], [1], [-1], [-1], [1], [1], [-1], [-1], [1], [1], [1], [1], [1], [-1], [-1], [1], [1], [-1], [-1], [1], [-1], [1], [1], [-1], [-1], [-1], [-1], [-1], [-1], [1], [-1], [-1], [-1], [-1], [1], [1], [-1], [-1], [1], [-1], [-1], [1], [1], [1], [-1], [-1], [-1], [-1], [1], [1], [1], [1], [-1], [-1], [1], [1], [1], [-1], [1], [-1], [1], [1], [1], [-1], [-1], [1], [-1], [1], [1], [1], [1], [1], [1], [1], [-1], [-1], [1], [1], [-1], [1], [-1], [-1], [-1], [-1], [1], [1], [-1], [-1], [1], [-1], [1], [-1], [1], [1], [1], [1], [-1], [-1], [1], [-1], [-1], [1], [1], [-1], [1], [1], [1], [-1], [1], [-1], [1], [-1], [-1], [-1], [1], [1], [1], [-1], [-1], [-1], [1], [1], [1], [-1], [-1], [1], [-1], [-1], [1], [-1], [1], [1], [1], [1], [1], [1], [1], [-1], [-1], [-1], [-1], [1], [1], [-1], [-1], [-1], [1], [-1], [-1], [1], [1], [1], [1], [1], [-1], [-1], [1], [1], [1], [-1], [-1], [-1], [1], [1], [1], [-1], [-1], [1], [1], [-1], [1], [1], [1], [1], [-1], [-1], [1], [1], [-1], [1], [1], [1], [1], [1], [1], [1], [-1], [1], [-1], [1], [-1], [1], [-1], [1], [-1], [-1], [-1], [1], [-1], [-1], [-1], [-1], [1], [1], [-1], [1], [-1], [-1], [-1], [1], [1], [-1], [1], [1], [1], [-1], [1], [-1], [-1], [-1], [-1], [-1], [-1], [1], [1], [-1], [1], [1], [1], [1], [-1], [-1], [1], [-1], [1], [-1], [-1], [-1], [1], [1], [1], [-1], [-1], [-1], [1], [-1], [1], [1], [-1], [1], [-1], [1], [-1], [1], [1], [-1], [-1], [-1], [-1], [-1], [-1], [1], [1], [1], [1], [1], [-1], [-1], [-1], [1], [1], [1], [1], [-1], [1], [1], [1], [-1], [-1], [-1], [-1], [1], [1], [1], [1], [1], [1], [1], [1], [-1], [-1], [-1], [1], [-1], [-1], [1], [1], [-1], [-1], [-1], [-1], [1], [1], [-1], [-1], [-1], [1], [-1], [1], [1], [1], [-1], [-1], [-1], [-1], [1], [-1], [-1], [1], [-1], [1], [1], [1], [-1], [-1], [1], [-1], [-1], [1], [1], [1], [-1], [-1], [1], [1], [-1], [-1], [1], [1], [1], [1], [-1], [1], [1], [-1], [1], [-1], [1], [-1], [-1], [-1], [1], [1], [-1], [-1], [1], [1], [1], [-1], [1], [1], [1], [-1], [1], [-1], [-1], [1], [-1], [-1], [-1], [-1], [1], [1], [1], [-1], [1], [1], [-1], [1], [-1], [-1], [-1], [1], [1], [-1], [1], [-1], [1], [-1], [-1], [1], [1], [1], [-1], [-1], [1], [1], [-1], [-1], [-1], [1], [1], [-1], [1], [-1], [-1], [-1], [1], [-1], [-1], [1], [1], [-1], [1], [-1], [-1], [1], [1], [1], [1], [1], [-1], [-1], [1], [1], [1], [-1], [1], [-1], [-1], [-1], [1], [-1], [-1], [1], [1], [-1], [-1], [1], [-1], [-1], [1], [-1], [1], [-1], [1], [-1], [-1], [-1], [-1], [1], [-1], [-1], [1], [-1], [-1], [1], [1], [-1], [1], [-1], [-1], [-1], [-1], [1], [1], [1], [1], [-1], [1], [1], [1], [1], [-1], [1], [-1], [1], [1], [-1], [-1], [1], [-1], [-1], [-1], [-1], [-1], [1], [1], [1], [-1], [-1], [1], [1], [1], [1], [-1], [1], [1], [-1], [-1], [-1], [1], [1], [-1], [-1], [1], [-1], [-1], [-1], [1], [-1], [-1], [1], [-1], [-1], [1], [-1], [-1], [1], [1], [-1], [-1], [-1], [1], [1], [1], [1], [1], [1], [1], [-1], [1], [-1], [-1], [1], [1], [1], [-1], [1], [-1], [-1], [1], [1], [-1], [-1], [1], [1], [-1], [-1], [-1], [1], [-1], [1], [-1], [-1], [1], [-1], [-1], [-1], [1], [1], [1], [1], [-1], [1], [1], [1], [-1], [-1], [1], [-1], [-1], [-1], [1], [-1], [-1], [1], [-1], [-1], [-1], [1], [1], [1], [-1], [1], [-1], [-1], [-1], [1], [1], [-1], [1], [-1], [1], [-1], [-1], [-1], [-1], [1], [-1], [-1], [1], [-1], [-1], [1], [-1], [1], [-1], [-1], [1], [-1], [-1], [-1], [-1], [-1], [1], [-1], [1], [1], [1], [1], [-1], [-1], [-1], [-1], [1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [1], [1], [-1], [1], [-1], [-1], [1], [1], [-1], [-1], [-1], [-1], [-1], [1], [1], [1], [1], [-1], [-1], [-1], [-1], [-1], [1], [1], [1], [1], [1], [-1], [1], [-1], [1], [1], [-1], [1], [-1], [-1], [1], [1], [-1], [-1], [1], [-1], [-1], [-1], [-1], [1], [-1], [-1], [1], [1], [1], [-1], [1], [-1], [1], [1], [1], [1], [1], [-1], [-1], [-1], [-1], [-1], [1], [1], [-1], [1], [1], [1], [1], [-1], [1], [1], [-1], [-1], [-1], [1], [-1]]]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected time_distributed_17 to have shape (1, 1) but got array with shape (1500, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-fe4e57ddde2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m#Entrainement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtbCallBack\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m#Accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected time_distributed_17 to have shape (1, 1) but got array with shape (1500, 1)"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "#on fixe la seed utilisée par LSTM()\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "\n",
    "y_tr = [y_train.values.reshape(1, y_train.shape[0], 1).tolist()]\n",
    "y_ts = [y_test.values.reshape(1, y_test.shape[0], 1).tolist()]\n",
    "\n",
    "X_tr = X_train_dtm.toarray()\n",
    "X_ts = X_test_dtm.toarray()\n",
    "\n",
    "X_tr = np.reshape(X_tr, X_tr.shape + (1,))\n",
    "X_ts = np.reshape(X_ts, X_ts.shape + (1,))\n",
    "\n",
    "\n",
    "X_tr = np.transpose(X_tr, (0, 2, 1))\n",
    "X_ts = np.transpose(X_ts, (0, 2, 1))\n",
    "\n",
    "print(X_tr.shape)\n",
    "print(y_tr)\n",
    "\n",
    "#https://stackoverflow.com/questions/44273249/in-keras-what-exactly-am-i-configuring-when-i-create-a-stateful-lstm-layer-wi\n",
    "\n",
    "#dans un terminal, nous pouvons ensuite lancer tensorboard de cette façon:\n",
    "#    tensorboard --logdir path_to_current_dir/Graph \n",
    "\n",
    "#on crée le tensorflow callback pour la visualisation dans le terminal avec tensorboard\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=X_tr.shape[1:], merge_mode='concat'))\n",
    "model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#Entrainement\n",
    "for epoch in range(250):\n",
    "    model.fit(X_tr, y_tr, epochs=1, batch_size=1, verbose=2, callbacks=[tbCallBack])\n",
    "    \n",
    "#Accuracy\n",
    "score = model.evaluate(X_ts, y_ts, batch_size=1)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
